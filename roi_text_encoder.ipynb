{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0678cb22-26ed-4b31-8f9c-01df86152ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hdd/yuchen\n"
     ]
    }
   ],
   "source": [
    "%cd /hdd/yuchen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6efe314-dd30-40f3-a29e-7f72efaa9c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, BatchSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import nltk, re, string, collections\n",
    "import scipy \n",
    "import IProgress \n",
    "from ipywidgets import FloatProgress\n",
    "from nilearn import connectome\n",
    "from sklearn.preprocessing import Normalizer, OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from torch.nn import Linear, ReLU, Dropout\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, TopKPooling, BatchNorm, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7784657d-f060-432a-af91-671b7d40eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c7c0ca5-ed95-47b9-9e61-18f8916c7749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2054f6cb12e466686ba944a32a0fa76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 stims not found\n"
     ]
    }
   ],
   "source": [
    "featname = 'c'\n",
    "use_stim = 'each'\n",
    "\n",
    "subject= 'subj01'\n",
    "topdir = '/hdd/yuchen/cs292/nsdfeat'\n",
    "\n",
    "savedir = f'{topdir}/subjfeat/'\n",
    "featdir = f'{topdir}/15dim/{featname}/'\n",
    "\n",
    "nsd_expdesign = scipy.io.loadmat('/hdd/yuchen/cs292/nsd/nsddata/experiments/nsd/nsd_expdesign.mat')\n",
    "\n",
    "# Note that most of them are 1-base index!\n",
    "sharedix = nsd_expdesign['sharedix'] -1 \n",
    "stims = np.load(f'/hdd/yuchen/cs292/mrifeat/{subject}/{subject}_stims.npy')\n",
    "\n",
    "feats = []\n",
    "tr_idx = np.zeros(len(stims))\n",
    "\n",
    "ct = 0\n",
    "\n",
    "for idx, s in tqdm(enumerate(stims)): \n",
    "    if s in sharedix:\n",
    "        tr_idx[idx] = 0\n",
    "    else:\n",
    "        tr_idx[idx] = 1    \n",
    "    if os.path.exists(f'{featdir}/{s:06}.npy'):\n",
    "        feat = np.load(f'{featdir}/{s:06}.npy')\n",
    "        feats.append(feat)\n",
    "    else: \n",
    "        ct += 1\n",
    "\n",
    "feats = np.stack(feats)    \n",
    "\n",
    "feats_tr = feats[tr_idx==1]\n",
    "feats_te = feats[tr_idx==0]\n",
    "\n",
    "print(ct, 'stims not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d31d80d1-ca7c-42ad-9a27-fcf8d4d2f602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FFC', 'FOP1', 'FOP2', 'FOP3', 'FOP4', 'FOP5', 'IPS0', 'IPS1',\n",
       "       'IPS2', 'IPS3', 'IPS4', 'IPS5', 'LO1', 'LO2', 'LO3', 'MST', 'MT',\n",
       "       'OFC', 'OP1', 'OP2-3', 'OP4', 'PF', 'PFcm', 'PFm', 'PFop', 'PHC1',\n",
       "       'PHC2', 'RSC', 'TO1', 'TO2', 'V6', 'V6A', 'V7', 'V8', 'VMV1',\n",
       "       'VMV2', 'VMV3', 'VO1', 'VO2', 'VVC', 'hV4', 'pOFC'], dtype='<U5')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "dir = '/hdd/yuchen/cs292/mrifeat/subj01/'\n",
    "exclude_lst = ['lateral', 'midlateral', 'midparietal', 'midventral','early',\n",
    "              'V1', 'V2', 'V3', 'V3A', 'V3B', 'V3CD', 'V4', 'V4t',\n",
    "               'stims', 'stims.npy',\n",
    "               'PFt',\n",
    "              ]\n",
    "# https://github.com/ReedOnePeck/MindDiffuser/blob/main/Feature%20decoding/Semantic_feature_decoding.py\n",
    "include_lst = ['VO','PHC','MT','MST','LO','IPS','hV4', 'OFC', 'FFC', 'FOP', 'PF', 'V6', 'V7', 'V8', 'RSC','TO','OP','VMV','VVC']\n",
    "\n",
    "lst = []\n",
    "lst_all = []\n",
    "for file_path in glob.glob(f\"{dir}/*.npy\"):\n",
    "    area = file_path.split('/')[-1].split('_')[1]\n",
    "    for key in include_lst:\n",
    "        if key in area and area not in exclude_lst: \n",
    "            lst.append(area)\n",
    "    lst_all.append(area)\n",
    "            # pass\n",
    "    # if area in include_lst:\n",
    "    # lst.append(area)\n",
    "lst = np.unique(np.array(lst))\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6415cb43-504d-4785-a062-126a49e6b284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '10d', '10pp', '10r', '10v', '11l', '13l', '2', '23c', '23d',\n",
       "       '24dd', '24dv', '25', '31a', '31pd', '31pv', '33pr', '3a', '3b',\n",
       "       '4', '43', '44', '45', '46', '47l', '47m', '47s', '52', '55b',\n",
       "       '5L', '5m', '5mv', '6a', '6d', '6ma', '6mp', '6r', '6v', '7AL',\n",
       "       '7Am', '7PC', '7PL', '7Pm', '7m', '8Ad', '8Av', '8BL', '8BM', '8C',\n",
       "       '9-46d', '9a', '9m', '9p', 'A1', 'A4', 'A5', 'AAIC', 'AIP', 'AVI',\n",
       "       'DVT', 'EC', 'FEF', 'FFC', 'FOP1', 'FOP2', 'FOP3', 'FOP4', 'FOP5',\n",
       "       'FST', 'H', 'IFJa', 'IFJp', 'IFSa', 'IFSp', 'IP0', 'IP1', 'IP2',\n",
       "       'IPS0', 'IPS1', 'IPS2', 'IPS3', 'IPS4', 'IPS5', 'Ig', 'LBelt',\n",
       "       'LIPd', 'LIPv', 'LO1', 'LO2', 'LO3', 'MBelt', 'MI', 'MIP', 'MST',\n",
       "       'MT', 'OFC', 'OP1', 'OP2-3', 'OP4', 'PBelt', 'PCV', 'PEF', 'PF',\n",
       "       'PFcm', 'PFm', 'PFop', 'PFt', 'PGi', 'PGp', 'PGs', 'PH', 'PHA1',\n",
       "       'PHA2', 'PHA3', 'PHC1', 'PHC2', 'PHT', 'PI', 'PIT', 'POS1', 'POS2',\n",
       "       'PSL', 'PeEc', 'Pir', 'PoI1', 'PoI2', 'PreS', 'ProS', 'RI', 'RSC',\n",
       "       'SCEF', 'SFL', 'SPL1', 'STGa', 'STSda', 'STSdp', 'STSva', 'STSvp',\n",
       "       'STV', 'TA2', 'TE1a', 'TE1m', 'TE1p', 'TE2a', 'TE2p', 'TF', 'TGd',\n",
       "       'TGv', 'TO1', 'TO2', 'TPOJ1', 'TPOJ2', 'TPOJ3', 'V1', 'V1d', 'V1v',\n",
       "       'V2', 'V2d', 'V2v', 'V3', 'V3A', 'V3B', 'V3CD', 'V3d', 'V3v', 'V4',\n",
       "       'V4t', 'V6', 'V6A', 'V7', 'V8', 'VIP', 'VMV1', 'VMV2', 'VMV3',\n",
       "       'VO1', 'VO2', 'VVC', 'a10p', 'a24', 'a24pr', 'a32pr', 'a47r',\n",
       "       'a9-46v', 'd23ab', 'd32', 'early', 'hV4', 'i6-8', 'lateral',\n",
       "       'midlateral', 'midparietal', 'midventral', 'p10p', 'p24', 'p24pr',\n",
       "       'p32', 'p32pr', 'p47r', 'p9-46v', 'pOFC', 'parietal', 's32',\n",
       "       's6-8', 'stims', 'stims.npy', 'v23ab', 'ventral'], dtype='<U11')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(lst_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6307b28-f18f-498c-8d3b-ca442f5d6d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 / 41\r"
     ]
    }
   ],
   "source": [
    "savedir = '/hdd/yuchen/cs292/mrifeat/subj01/'\n",
    "# roi = 'V1'\n",
    "ct = 0\n",
    "roi = lst[0]\n",
    "\n",
    "betas_tr = np.load(f'{savedir}/{subject}_{roi}_betas_tr.npy')\n",
    "betas_te = np.load(f'{savedir}/{subject}_{roi}_betas_te.npy')\n",
    "betas_tr_avg = np.load(f'{savedir}/{subject}_{roi}_betas_ave_tr.npy')\n",
    "betas_te_avg = np.load(f'{savedir}/{subject}_{roi}_betas_ave_te.npy')\n",
    "\n",
    "num_trials_tr, num_neurons = betas_tr.shape\n",
    "num_trials_te_avg, _ = betas_te_avg.shape\n",
    "roi_neuron = {roi: num_neurons}\n",
    "\n",
    "for roi in lst[1:]:\n",
    "    ct += 1\n",
    "    print(ct,'/',len(lst)-1, end='\\r')\n",
    "    original_array = np.load(f'{savedir}/{subject}_{roi}_betas_tr.npy')\n",
    "    roi_neuron[roi] = original_array.shape[1]\n",
    "    betas_tr = np.hstack([betas_tr,original_array])\n",
    "    original_array = np.load(f'{savedir}/{subject}_{roi}_betas_ave_te.npy')\n",
    "    betas_te_avg = np.hstack([betas_te_avg,original_array])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb74d48f-1e1c-490d-a6cf-9970435e5319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad00efbdb054166bc7f5bade1d43470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stims = np.load(f'/hdd/yuchen/cs292/mrifeat/{subject}/{subject}_stims_ave.npy')\n",
    "\n",
    "feats_avg = []\n",
    "tr_idx = np.zeros(len(stims))\n",
    "\n",
    "ct = 0\n",
    "\n",
    "for idx, s in tqdm(enumerate(stims)): \n",
    "    if s in sharedix:\n",
    "        tr_idx[idx] = 0\n",
    "    else:\n",
    "        tr_idx[idx] = 1    \n",
    "    if os.path.exists(f'{featdir}/{s:06}.npy'):\n",
    "        feat = np.load(f'{featdir}/{s:06}.npy')\n",
    "        feats_avg.append(feat)\n",
    "    else: \n",
    "        print(f'{s:06}.npy does not exist')\n",
    "        feats_avg.append(np.array([0]*768*15))\n",
    "        ct += 1\n",
    "\n",
    "feats_avg = np.stack(feats_avg)    \n",
    "\n",
    "feats_tr_avg = feats_avg[tr_idx==1]\n",
    "feats_te_avg = feats_avg[tr_idx==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5879e77-75d1-484e-907c-e9c9202de0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feats_tr (24980, 11520)\n",
      "betas_tr (24980, 15895)\n",
      "feats_te_avg (982, 11520)\n",
      "betas_te_avg (982, 15895)\n"
     ]
    }
   ],
   "source": [
    "print('feats_tr', feats_tr.shape)  # num_trials * num_latent_feat\n",
    "# print('feats_te', feats_te.shape)\n",
    "print('betas_tr', betas_tr.shape)  # num_trials * num_neurons\n",
    "# print('betas_te', betas_te.shape)\n",
    "\n",
    "# print('feats_tr_avg', feats_tr_avg.shape)\n",
    "print('feats_te_avg', feats_te_avg.shape)  # num_avg_trials * num_latent_feat\n",
    "# print('betas_tr_avg', betas_tr_avg.shape)\n",
    "print('betas_te_avg', betas_te_avg.shape)  # num_avg_trials * num_neurons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f2033d7-b18d-4f15-af59-86fb5ff57919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FFC': 747,\n",
       " 'FOP1': 208,\n",
       " 'FOP2': 134,\n",
       " 'FOP3': 155,\n",
       " 'FOP4': 410,\n",
       " 'FOP5': 393,\n",
       " 'IPS0': 606,\n",
       " 'IPS1': 462,\n",
       " 'IPS2': 500,\n",
       " 'IPS3': 508,\n",
       " 'IPS4': 65,\n",
       " 'IPS5': 14,\n",
       " 'LO1': 343,\n",
       " 'LO2': 193,\n",
       " 'LO3': 218,\n",
       " 'MST': 224,\n",
       " 'MT': 212,\n",
       " 'OFC': 1029,\n",
       " 'OP1': 227,\n",
       " 'OP2-3': 187,\n",
       " 'OP4': 523,\n",
       " 'PF': 1103,\n",
       " 'PFcm': 299,\n",
       " 'PFm': 1786,\n",
       " 'PFop': 454,\n",
       " 'PHC1': 199,\n",
       " 'PHC2': 183,\n",
       " 'RSC': 401,\n",
       " 'TO1': 284,\n",
       " 'TO2': 55,\n",
       " 'V6': 394,\n",
       " 'V6A': 223,\n",
       " 'V7': 186,\n",
       " 'V8': 330,\n",
       " 'VMV1': 324,\n",
       " 'VMV2': 245,\n",
       " 'VMV3': 203,\n",
       " 'VO1': 216,\n",
       " 'VO2': 313,\n",
       " 'VVC': 482,\n",
       " 'hV4': 473,\n",
       " 'pOFC': 384}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roi_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fafd8421-9704-4e84-80fc-a32cda7ade94",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_tr_ = (betas_tr - betas_tr.mean()) / betas_tr.std()\n",
    "betas_tr_avg_ = (betas_tr_avg - betas_tr_avg.mean()) / betas_tr_avg.std()\n",
    "betas_te_ = (betas_te - betas_te.mean()) / betas_te.std()\n",
    "betas_te_avg_ = (betas_te_avg - betas_te_avg.mean()) / betas_te_avg.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59add481-fe29-4b65-a549-64865040f28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24980, 15895), (982, 15895))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas_tr_.shape, betas_te_avg_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e180a496-c301-4b91-b54a-7573d02ff5ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## goal: 0.26555938"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "389b5d54-aacc-4db9-9c1f-98ff7af7823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os\n",
    "import numpy as np\n",
    "from himalaya.backend import set_backend\n",
    "from himalaya.ridge import RidgeCV\n",
    "from himalaya.scoring import correlation_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "alpha = [0.000001,0.00001,0.0001,0.001,0.01, 0.1, 1]\n",
    "\n",
    "ridge = RidgeCV(alphas=alpha)\n",
    "\n",
    "preprocess_pipeline = make_pipeline(\n",
    "        StandardScaler(with_mean=True, with_std=True),\n",
    "    )\n",
    "pipeline = make_pipeline(\n",
    "        preprocess_pipeline,\n",
    "        ridge,\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7914f467-bc01-4c6b-bd24-312df879bfb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.753467549664157"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = betas_tr_\n",
    "Y = feats_tr\n",
    "X_te = betas_te_avg_\n",
    "pipeline.fit(X, Y)\n",
    "scores = pipeline.predict(X_te)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(scores,feats_te_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da88f155-b5ed-4fa5-9e1e-b399a746db76",
   "metadata": {},
   "source": [
    "## gcn best: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "89005bec-c620-4078-a34b-7a55dc0c1e04",
   "metadata": {},
   "source": [
    "ideas inspired by:\n",
    "https://www.mdpi.com/2076-3425/12/10/1394\n",
    "- Decoding Visual fMRI Stimuli from Human Brain Based on Graph Convolutional Neural Network\n",
    "https://www.bendsawyer.com/wp-content/uploads/2023/07/Saeidi-et-al-Decoding-Task-Based-fMRI-Data-with-Graph-Neural-Networks-Considering-Individual-Differences.pdf\n",
    "- Decoding Task-Based fMRI Data with Graph Neural Networks, Considering Individual Differences\n",
    "\n",
    "code adapted from:\n",
    "https://github.com/krzysztoffiok/gnn-classification-pipeline/blob/master/gnn_fw/models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9a9c55c-22e0-4a55-9c94-51049489f2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_combined_neuron = {}\n",
    "neuron = 0\n",
    "\n",
    "for roi_idx in range(len(roi_neuron)):\n",
    "    if roi_idx == 1: roi_combined_neuron['FOP'] = 0\n",
    "    roi_name = list(roi_neuron.keys())[roi_idx]\n",
    "    if 'FOP' in roi_name:\n",
    "        if roi_name == 'FOP1':\n",
    "            roi_combined_neuron['FOP'] = 0\n",
    "            for name_idx in range(1,6):\n",
    "                roi_combined_neuron['FOP'] += roi_neuron['FOP'+str(name_idx)] \n",
    "    elif 'IPS' in roi_name:\n",
    "        if roi_name == 'IPS0':\n",
    "            roi_combined_neuron['IPS01'] = 0\n",
    "            roi_combined_neuron['IPS25'] = 0\n",
    "            for name_idx in range(0,6):\n",
    "                if name_idx <=1: roi_combined_neuron['IPS01'] += roi_neuron['IPS'+str(name_idx)] \n",
    "                else: roi_combined_neuron['IPS25'] += roi_neuron['IPS'+str(name_idx)] \n",
    "\n",
    "    elif 'LO' in roi_name:\n",
    "        if roi_name == 'LO1':\n",
    "            roi_combined_neuron['LO'] = 0\n",
    "            for name_idx in range(1,4):\n",
    "                roi_combined_neuron['LO'] += roi_neuron['LO'+str(name_idx)]  \n",
    "\n",
    "    elif roi_name in ['OP1', 'OP2-3', 'OP4']:\n",
    "        if roi_name=='OP1': roi_combined_neuron['OP'] = roi_neuron[roi_name]\n",
    "        else: roi_combined_neuron['OP'] += roi_neuron[roi_name]\n",
    "\n",
    "    elif 'PHC' in roi_name:\n",
    "        if roi_name == 'PHC1':\n",
    "            roi_combined_neuron['PHC'] = 0\n",
    "            for name_idx in range(1,3):\n",
    "                roi_combined_neuron['PHC'] += roi_neuron['PHC'+str(name_idx)]  \n",
    "\n",
    "    elif roi_name in ['TO1', 'TO2']:\n",
    "        if roi_name=='TO1': roi_combined_neuron['TO'] = roi_neuron[roi_name]\n",
    "        else: roi_combined_neuron['TO'] += roi_neuron[roi_name]\n",
    "\n",
    "    elif roi_name in ['V6', 'V6A', 'V7', 'V8']:\n",
    "        if roi_name=='V6': roi_combined_neuron['V68'] = roi_neuron[roi_name]\n",
    "        else: roi_combined_neuron['V68'] += roi_neuron[roi_name]\n",
    "    \n",
    "    elif roi_name in ['VMV1', 'VMV2', 'VMV3']:\n",
    "        if roi_name=='VMV1': roi_combined_neuron['VMV'] = roi_neuron[roi_name]\n",
    "        else: roi_combined_neuron['VMV'] += roi_neuron[roi_name]    \n",
    "\n",
    "    elif roi_name in ['VO1', 'VO2']:\n",
    "        if roi_name=='VO1': roi_combined_neuron['VO'] = roi_neuron[roi_name]\n",
    "        else: roi_combined_neuron['VO'] += roi_neuron[roi_name] \n",
    "            \n",
    "    else:\n",
    "        roi_combined_neuron[roi_name] = roi_neuron[roi_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc40c977-1092-4566-a51e-6e94c9b63a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FFC': 747,\n",
       " 'FOP': 1300,\n",
       " 'IPS01': 1068,\n",
       " 'IPS25': 1087,\n",
       " 'LO': 754,\n",
       " 'MST': 224,\n",
       " 'MT': 212,\n",
       " 'OFC': 1029,\n",
       " 'OP': 937,\n",
       " 'PF': 1103,\n",
       " 'PFcm': 299,\n",
       " 'PFm': 1786,\n",
       " 'PFop': 454,\n",
       " 'PHC': 382,\n",
       " 'RSC': 401,\n",
       " 'TO': 339,\n",
       " 'V68': 1133,\n",
       " 'VMV': 772,\n",
       " 'VO': 529,\n",
       " 'VVC': 482,\n",
       " 'hV4': 473,\n",
       " 'pOFC': 384}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roi_combined_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6de3a37-794d-4d10-91f7-e046c8994213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], 1786)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude_lst = []\n",
    "max_neuron_afer_excluding = 0\n",
    "\n",
    "for idx in range(len(roi_combined_neuron)):\n",
    "    num_temp = list(roi_combined_neuron.values())[idx] \n",
    "    if num_temp > 1300 or num_temp < 400: exclude_lst.append(list(roi_combined_neuron.keys())[idx] )\n",
    "    else: \n",
    "        if num_temp > max_neuron_afer_excluding: max_neuron_afer_excluding = num_temp\n",
    "\n",
    "# exclude nothing\n",
    "exclude_lst = []\n",
    "max_neuron_afer_excluding = max(roi_combined_neuron.values())\n",
    "\n",
    "exclude_lst , max_neuron_afer_excluding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e58aeb7-d2e0-4b5d-b0d2-c3cdd72c5e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FFC': 747,\n",
       " 'FOP': 1300,\n",
       " 'IPS01': 1068,\n",
       " 'IPS25': 1087,\n",
       " 'LO': 754,\n",
       " 'MST': 224,\n",
       " 'MT': 212,\n",
       " 'OFC': 1029,\n",
       " 'OP': 937,\n",
       " 'PF': 1103,\n",
       " 'PFcm': 299,\n",
       " 'PFm': 1786,\n",
       " 'PFop': 454,\n",
       " 'PHC': 382,\n",
       " 'RSC': 401,\n",
       " 'TO': 339,\n",
       " 'V68': 1133,\n",
       " 'VMV': 772,\n",
       " 'VO': 529,\n",
       " 'VVC': 482,\n",
       " 'hV4': 473,\n",
       " 'pOFC': 384}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roi_combined_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "981fa705-082b-4bbe-9751-a5d25b12ba7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOP (24980, 1300)\n",
      "IPS01 (24980, 1068)\n",
      "IPS25 (24980, 1087)\n",
      "LO (24980, 754)\n",
      "MST (24980, 224)\n",
      "MT (24980, 212)\n",
      "OFC (24980, 1029)\n",
      "OP (24980, 937)\n",
      "PF (24980, 1103)\n",
      "PFcm (24980, 299)\n",
      "PFm (24980, 1786)\n",
      "PFop (24980, 454)\n",
      "PHC (24980, 382)\n",
      "RSC (24980, 401)\n",
      "TO (24980, 339)\n",
      "V68 (24980, 1133)\n",
      "VMV (24980, 772)\n",
      "VO (24980, 529)\n",
      "VVC (24980, 482)\n",
      "hV4 (24980, 473)\n",
      "pOFC (24980, 384)\n",
      "22 / 22\r"
     ]
    }
   ],
   "source": [
    "roi_neuron = roi_combined_neuron\n",
    "max_num_neurons = max_neuron_afer_excluding\n",
    "original_betas_tr = []\n",
    "original_betas_te_avg = []\n",
    "\n",
    "betas_tr_temp = betas_tr_[:, :list(roi_combined_neuron.values())[0]]\n",
    "original_betas_tr.append(betas_tr_temp)\n",
    "zeros = np.zeros((betas_tr_.shape[0], max_num_neurons))\n",
    "zeros[:, :betas_tr_temp.shape[-1]] = betas_tr_temp\n",
    "betas_tr_temp = zeros\n",
    "\n",
    "for roi in range(2, len(list(roi_combined_neuron.keys())[1:]) + 2):\n",
    "    print(roi, '/', len(list(roi_combined_neuron.keys())[1:])+1, end='\\r')\n",
    "    name = list(roi_combined_neuron.keys())[roi-1]\n",
    "    if name not in exclude_lst: \n",
    "        temp = betas_tr_[:, sum(list(roi_combined_neuron.values())[:roi-1]):sum(list(roi_combined_neuron.values())[:roi])]\n",
    "        print(name, temp.shape)\n",
    "        original_betas_tr.append(temp)\n",
    "        zeros = np.zeros((betas_tr_.shape[0], max_num_neurons))\n",
    "        zeros[:, :temp.shape[-1]] = temp\n",
    "        betas_tr_temp = np.hstack([betas_tr_temp, zeros])\n",
    "\n",
    "betas_te_avg_temp = betas_te_avg_[:, :list(roi_combined_neuron.values())[0]]\n",
    "original_betas_te_avg.append(betas_te_avg_temp)\n",
    "zeros = np.zeros((betas_te_avg_.shape[0], max_num_neurons))\n",
    "zeros[:, :betas_te_avg_temp.shape[-1]] = betas_te_avg_temp\n",
    "betas_te_avg_temp = zeros\n",
    "for roi in range(2, len(list(roi_combined_neuron.keys())[1:]) + 2):\n",
    "    print(roi, '/', len(list(roi_combined_neuron.keys())[1:])+1, end='\\r')\n",
    "    name = list(roi_combined_neuron.keys())[roi-1]\n",
    "    if name not in exclude_lst: \n",
    "        temp = betas_te_avg_[:, sum(list(roi_combined_neuron.values())[:roi-1]):sum(list(roi_combined_neuron.values())[:roi])]\n",
    "        original_betas_te_avg.append(temp)\n",
    "        zeros = np.zeros((betas_te_avg_.shape[0], max_num_neurons))\n",
    "        zeros[:, :temp.shape[-1]] = temp\n",
    "        betas_te_avg_temp = np.hstack([betas_te_avg_temp, zeros])\n",
    "\n",
    "betas_tr_ = betas_tr_temp\n",
    "betas_te_avg_ = betas_te_avg_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cff1b534-2f9f-4d55-887c-e9791b56d846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24980, 39292), (982, 39292))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas_tr_.shape, betas_te_avg_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dbb551d-f924-4b47-b4f5-9f9d111b33a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39292\n",
      "39292\n"
     ]
    }
   ],
   "source": [
    "print(betas_tr_.shape[-1])\n",
    "print(max_neuron_afer_excluding * (len(roi_neuron) - len(exclude_lst)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7785af68-f500-4e10-8b37-0643b35dd7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_roi = len(roi_neuron) - len(exclude_lst)\n",
    "# num_roi = 4\n",
    "\n",
    "num_neurons = betas_tr_.shape[-1]\n",
    "num_pad = (num_neurons // num_roi + 1) * num_roi - num_neurons\n",
    "\n",
    "betas_tr = np.pad(betas_tr_, ((0, 0), (0, num_pad)), 'constant', constant_values=0)\n",
    "betas_tr_avg = np.pad(betas_tr_avg_, ((0, 0), (0, num_pad)), 'constant', constant_values=0)\n",
    "betas_te = np.pad(betas_te_, ((0, 0), (0, num_pad)), 'constant', constant_values=0)\n",
    "betas_te_avg = np.pad(betas_te_avg_, ((0, 0), (0, num_pad)), 'constant', constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73e6d383-91be-41ef-bd97-0ff64bb0bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons_each_roi = int(betas_te_avg.shape[-1]/num_roi)\n",
    "\n",
    "# conn_measure = connectome.ConnectivityMeasure(kind='correlation')\n",
    "\n",
    "# function_con = betas_tr.reshape(-1, num_roi, num_neurons_each_roi)\n",
    "\n",
    "# connectivity = conn_measure.fit_transform([np.mean(function_con, axis=-1)])\n",
    "# for i in range(len(connectivity[0])):\n",
    "#     connectivity[0][i, i] = 0  # setting the i-th row to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f955ed25-433e-4a7d-a4a1-4c521f928de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation_matrix = np.zeros((dim, dim))\n",
    "# correlation_matrix_m1 = np.zeros((dim, dim))\n",
    "# correlation_matrix_m2 = np.zeros((dim, dim))\n",
    "# correlation_matrix_p1 = np.zeros((dim, dim))\n",
    "# correlation_matrix_p2 = np.zeros((dim, dim))\n",
    "# edge_map = np.zeros((dim, dim))\n",
    "\n",
    "# for i in range(dim):\n",
    "#     for j in range(dim):\n",
    "#         correlation_matrix[i, j] = np.corrcoef([temp_lst[i]], [temp_lst[j]])[0, 1]\n",
    "#         correlation_matrix_m1[i, j] = np.corrcoef([np.roll(temp_lst[i], shift=-1)], [temp_lst[j]])[0, 1]\n",
    "#         correlation_matrix_m2[i, j] = np.corrcoef([np.roll(temp_lst[i], shift=-2)], [temp_lst[j]])[0, 1]\n",
    "#         correlation_matrix_p1[i, j] = np.corrcoef([np.roll(temp_lst[i], shift=1)], [temp_lst[j]])[0, 1]\n",
    "#         correlation_matrix_p2[i, j] = np.corrcoef([np.roll(temp_lst[i], shift=2)], [temp_lst[j]])[0, 1]\n",
    "#     correlation_with_shift = np.vstack([correlation_matrix_p2[i], correlation_matrix_p1[i], \n",
    "#                                     correlation_matrix[i], \n",
    "#                                     correlation_matrix_m1[i], correlation_matrix_m2[i]])\n",
    "#     edge_map[i][np.where(np.argmax(correlation_with_shift, axis=0)>2)] = 1\n",
    "#     edge_map[i][np.where(np.argmax(correlation_with_shift, axis=0)<2)] = -1\n",
    "#     print(np.argmax(correlation_with_shift, axis=0))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c017a93-4a1c-4116-adb2-33a499525223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 24980)\n"
     ]
    }
   ],
   "source": [
    "temp_lst = []\n",
    "for i in original_betas_tr:\n",
    "    temp_lst.append(np.mean(i, axis=-1))\n",
    "temp_lst = np.array(temp_lst)\n",
    "print(temp_lst.shape)\n",
    "\n",
    "dim = temp_lst.shape[0]\n",
    "correlation_matrix = np.zeros((dim, dim))\n",
    "for i in range(dim):\n",
    "    for j in range(dim):\n",
    "        correlation_matrix[i, j] = np.corrcoef([temp_lst[i]], [temp_lst[j]])[0, 1]\n",
    "\n",
    "connectivity = correlation_matrix.reshape(1, dim, dim)\n",
    "\n",
    "for i in range(len(connectivity[0])):\n",
    "    connectivity[0][i, i] = 0  # setting the i-th row to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de5db6dc-10a8-4b04-b2f1-1fd87895f868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 22, 22)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connectivity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95a32b04-3d64-4c1d-a235-cb7a93795069",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "alpha = 0.05 / dim  \n",
    "\n",
    "edge_map = np.ones((dim, dim))\n",
    "\n",
    "for i in range(dim):\n",
    "    for j in range(dim):\n",
    "        area0 = np.mean(original_betas_tr[i], axis=1)\n",
    "        area1 = np.mean(original_betas_tr[j], axis=1)\n",
    "        area0 = np.diff(area0)[1:]\n",
    "        area1 = np.diff(area1)[1:]\n",
    "        # area0 = (np.diff(area0)[1:][1:] - np.diff(area0)[1:][:-1]) / 2\n",
    "        # area1 = (np.diff(area1)[1:][1:] - np.diff(area1)[1:][:-1]) / 2\n",
    "    \n",
    "        data = pd.DataFrame({ 'area0': area0,    'area1': area1 })\n",
    "        gc_res = grangercausalitytests(data, 1)\n",
    "        p = gc_res[1][0]['ssr_ftest'][1]\n",
    "        if p > alpha: edge_map[i, j] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40ee759e-4e0e-4dd2-903a-0ef1e6ffe576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 112)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjmatrix = np.where(abs(connectivity[0]) > 0.5)\n",
    "edge_index = np.array(adjmatrix)  # edge index matrix of shape [2, num_edges]\n",
    "\n",
    "edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33455771-3966-47d2-bd97-bf35f967253b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_attr = []\n",
    "for idx in range(edge_index.shape[1]):\n",
    "    edge_attr.append(connectivity[0][edge_index[0,idx], edge_index[1,idx]])\n",
    "edge_attr = np.array(edge_attr)\n",
    "edge_attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25b4e514-17c6-4b5a-a4f8-9efd2ad5506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da977361-3ae4-48a8-b19f-f0d554014529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove edge [0 5]\n",
      "remove edge [ 2 17]\n",
      "remove edge [6 0]\n",
      "remove edge [ 8 10]\n",
      "remove edge [ 9 10]\n",
      "remove edge [10  9]\n",
      "remove edge [12  8]\n",
      "remove edge [12 10]\n",
      "remove edge [15  0]\n",
      "remove edge [15 16]\n",
      "remove edge [16 18]\n"
     ]
    }
   ],
   "source": [
    "edge_index_temp = []\n",
    "edge_attr_temp = []\n",
    "for i in range(edge_attr.shape[0]):\n",
    "    edge = edge_index.T[i]\n",
    "    # if edge[0] < edge[1]:\n",
    "    if edge_map[edge[0], edge[1]] == True:\n",
    "        edge_index_temp.append(edge)\n",
    "        edge_attr_temp.append(edge_attr[i])\n",
    "    else: print('remove edge', edge)\n",
    "edge_attr = np.array(edge_attr_temp)\n",
    "edge_index = np.array(edge_index_temp).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea2ce094-8b66-4597-90f3-f9d4ad3bcd5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 101)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca9ec47e-90de-43ab-9a1c-6ab2dff4800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = feats_tr[0][:768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de0db021-bc55-46cb-9f18-c0515471db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trial_tr, num_trials_te = feats_tr.shape[0], feats_te_avg.shape[0]\n",
    "\n",
    "train_dataset = []\n",
    "\n",
    "for i in range(num_trial_tr):\n",
    "    train_dataset.append(Data(x=torch.tensor(betas_tr.reshape(-1, num_roi, num_neurons_each_roi)[i], dtype=torch.float), \n",
    "            edge_index=torch.tensor(edge_index, dtype=torch.long), \n",
    "            edge_attr=torch.tensor(edge_attr, dtype=torch.float),\n",
    "            y=torch.tensor(feats_tr[i][768:], dtype=torch.float)\n",
    "            ))\n",
    "    \n",
    "test_dataset = []\n",
    "for i in range(num_trials_te):\n",
    "    test_dataset.append(Data(x=torch.tensor(betas_te_avg.reshape(-1, num_roi, num_neurons_each_roi)[i], dtype=torch.float), \n",
    "            edge_index=torch.tensor(edge_index, dtype=torch.long), \n",
    "            edge_attr=torch.tensor(edge_attr, dtype=torch.float),\n",
    "            y=torch.tensor(feats_te_avg[i][768:], dtype=torch.float)\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15bd82d5-b3c7-4f5f-900f-92aef341a1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24980, 982)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20b580f4-8cac-42d4-9d7b-f7814758fddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(x=[22, 1787], edge_index=[2, 101], edge_attr=[101], y=[10752]),\n",
       " Data(x=[22, 1787], edge_index=[2, 101], edge_attr=[101], y=[10752]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0], test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3710c1eb-b263-4a19-94b7-e79b44b1c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ae53258-f5a4-48f6-a535-6399098bcde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, TopKPooling, BatchNorm,ChebConv, SAGEConv,GATConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool,global_add_pool\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, num_neurons_each_roi):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        hidden_dim = 512\n",
    "        \n",
    "        self.conv1 = ChebConv(num_neurons_each_roi, hidden_dim, K=4)\n",
    "        self.conv = ChebConv(hidden_dim, hidden_dim, K=3)\n",
    "        self.bn = BatchNorm(hidden_dim)\n",
    "\n",
    "        nnconv_channel = 16\n",
    "        \n",
    "        self.nnconv = nn.Sequential(\n",
    "            nn.Conv1d(1, nnconv_channel, kernel_size=1),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(nnconv_channel, nnconv_channel, kernel_size=3, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.downsample = nn.Conv1d(nnconv_channel, 1, kernel_size=1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024, 768*14)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch,edge_attr = data.x, data.edge_index, data.batch, data.edge_attr\n",
    "        batch_size = batch.max() + 1\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        \n",
    "        for i in range(1):\n",
    "            a = self.conv(x, edge_index, edge_attr)\n",
    "            a = self.bn(a)\n",
    "            x = x + F.leaky_relu(a)\n",
    "        \n",
    "        x = global_mean_pool(x, batch) # print(x.shape) torch.Size([64, 256])\n",
    "\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.nnconv(x)\n",
    "        for j in range(2):\n",
    "            x = x + self.net(x)\n",
    "        x = self.downsample(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7044222-7cce-44de-891b-481e01a5d892",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0] train loss 0.31548 val loss 0.29179\n",
      "[epoch 1] train loss 0.28341 val loss 0.27408\n",
      "[epoch 2] train loss 0.26725 val loss 0.26116\n",
      "[epoch 3] train loss 0.25214 val loss 0.25996\n",
      "[epoch 4] train loss 0.23761 val loss 0.26074\n",
      "[epoch 5] train loss 0.22391 val loss 0.2538\n",
      "[epoch 6] train loss 0.21146 val loss 0.24903\n",
      "[epoch 7] train loss 0.19977 val loss 0.25\n",
      "[epoch 8] train loss 0.19014 val loss 0.24659\n",
      "[epoch 9] train loss 0.18207 val loss 0.24989\n",
      "[epoch 10] train loss 0.17508 val loss 0.2509\n",
      "[epoch 11] train loss 0.16841 val loss 0.24897\n",
      "[epoch 12] train loss 0.16271 val loss 0.25194\n",
      "[epoch 13] train loss 0.15743 val loss 0.24914\n",
      "[epoch 14] train loss 0.15267 val loss 0.24908\n",
      "end 0.24659\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:1'\n",
    "model = GNN(num_neurons_each_roi=num_neurons_each_roi).to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 100\n",
    "patience = 5\n",
    "ct = 0\n",
    "best_loss_val = np.inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss_train = 0\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = batch.to(device)\n",
    "        batch_num = batch.batch.max() + 1\n",
    "        feats = batch['y'].reshape(batch_num,-1).to(device)\n",
    "        pred = model(batch)\n",
    "        \n",
    "        loss = loss_fn(pred, feats)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss_train += loss.item()\n",
    "    print('[epoch {}] train loss {}'.format(epoch, round(total_loss_train/len(train_dataloader), 5)), end = ' ')\n",
    "\n",
    "    model.eval()\n",
    "    total_loss_val = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            batch = batch.to(device)\n",
    "            batch_num = batch.batch.max() + 1\n",
    "            feats = batch['y'].reshape(batch_num,-1).to(device)\n",
    "            pred = model(batch)\n",
    "            loss = loss_fn(pred, feats)\n",
    "            total_loss_val += loss.item()\n",
    "        total_loss_val = total_loss_val/len(test_dataloader)\n",
    "        print('val loss {}'.format(round(total_loss_val, 5)))\n",
    "        \n",
    "        if total_loss_val < best_loss_val:\n",
    "            ct = 0\n",
    "            best_loss_val = total_loss_val\n",
    "            torch.save(model.state_dict(), '/hdd/yuchen/cs292/text_gcn_roi.pth')\n",
    "        else:\n",
    "            ct += 1\n",
    "            if ct > patience:\n",
    "                print('end', round(best_loss_val, 5))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfaf03c-30b6-454a-80e3-299a04ad6f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "cbd53405-fc3b-4813-b027-51579952c3e7",
   "metadata": {},
   "source": [
    "baseline: 0.26555938\n",
    "visual roi encoder: 0.25712526\n",
    "    combined brain \n",
    "        area 22 (0.5 edge threshold): 0.28 (0.3): 0.28532 (0.8): 0.28261\n",
    "        area 16 (0.5 edge): 0.2590016\n",
    "        area 22 (all visual, 0.5 edge threshold, random directed): 0.24708445\n",
    "        area 22 (all visual, 0.5 edge threshold, undirected): 0.28304416\n",
    "        area 22 (all visual, 0.5 edge threshold, undirected, 700-1300 neurons): 0.2582857580855489\n",
    "        area 22 (all visual, 0.5 edge threshold, undirected, 500-1300 neurons): 0.2577057983726263\n",
    "        area 22 (all visual, 0.5 edge threshold, undirected, 400-1300 neurons): 0.2555527025833726\n",
    "        area 22 (all visual, 0.5 edge threshold, undirected, 400-1100 neurons): 0.25652532558888197\n",
    "        area 22 (all visual, 0.5 edge threshold, undirected, 400-1300 neurons, better edge cor): 0.25573\n",
    "        area 22 (all visual, 0.5 edge threshold, undirected, better edge cor): 0.24720 (0.1 edge threshold) 0.2508\n",
    "            3 resnet block: 0.2477 (1): 0.2529  (4) 0.2479\n",
    "            three layers fc (512-1024-1024-dim) 0.2496 (512-256-1024-dim) 0.24896\n",
    "            hidden_dim=512; (128) bad; (256) 0.24996 (1024) 0.24749\n",
    "            nnconv_channel=4: 0.25028; (32) 0.24798\n",
    "        area 22 (all visual, 0.5 edge threshold, directed, better edge cor): \n",
    "            alpha=0.001: 0.24712 (0.05) 0.24632 (0.001, no multi-correction) 0.24718 (0.1) 0.24793 (0.05 with 1 dim) 0.24517\n",
    "            alpha=0.05 with 1 dim multi-correction:\n",
    "                relu: 0.24603\n",
    "                2 res block gcn: 0.24675\n",
    "                K == 3,3: worse; (5,3) 0.24868 (4,4) 0.24948 (4,5) 0.24979\n",
    "                (512-2048-dim) 0.24703 (512-512-dim) 0.24749\n",
    "                kernal5padding2: 0.25177 (10): 0.25208\n",
    "                no gcn res: 0.25825\n",
    "                0.7 thresh: 0.25228 (0.3) 0.25101 (0.6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f27f073-3a5d-4b98-9395-93b119dfb157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:1'\n",
    "model = GNN(num_neurons_each_roi=num_neurons_each_roi).to(device)\n",
    "model.load_state_dict(torch.load('/hdd/yuchen/cs292/text_gcn_roi.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fbb9afec-12d7-4c29-9c00-106dab3c4e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "lst = []\n",
    "lst_gt = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        batch = batch.to(device)\n",
    "        batch_num = batch.batch.max() + 1\n",
    "        pred = model(batch)\n",
    "        batch_num = batch.batch.max() + 1\n",
    "        feats = batch['y'].reshape(batch_num,-1).to(device)\n",
    "        \n",
    "        lst.append(pred.cpu().detach().numpy())\n",
    "        lst_gt.append(feats.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7be9ee0a-8e9a-447d-9283-2f3162c84c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = np.vstack(lst)\n",
    "lst_gt = np.vstack(lst_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15e3273d-fd0a-4d17-bfbb-0834707b3943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24607217"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(lst,lst_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d147279-ea3c-4d58-a90a-10a6bd489b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = lst.reshape(-1, 14, 768)\n",
    "cls_ = np.tile(cls, (982, 1))\n",
    "cls_ = np.expand_dims(cls_, 1)\n",
    "lst = np.concatenate((cls_, lst), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0b21fdcb-dd13-433f-b884-b83a64684a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(982, 15, 768)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19838a3b-5d2c-4686-a5fa-a6a57bdbcecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/hdd/yuchen/cs292/temp_lst_latent_c_roi.npy', lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016d1707-3fe3-4a67-b09e-60da0d13875a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d969aba0-cc4d-420a-a53d-d8bdefe7662e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6971274161290577"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse, os\n",
    "import numpy as np\n",
    "from himalaya.backend import set_backend\n",
    "from himalaya.ridge import RidgeCV\n",
    "from himalaya.scoring import correlation_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "alpha = [0.000001,0.00001,0.0001,0.001,0.01, 0.1, 1]\n",
    "\n",
    "ridge = RidgeCV(alphas=alpha)\n",
    "\n",
    "preprocess_pipeline = make_pipeline(\n",
    "        StandardScaler(with_mean=True, with_std=True),\n",
    "    )\n",
    "pipeline = make_pipeline(\n",
    "        preprocess_pipeline,\n",
    "        ridge,\n",
    ")    \n",
    "\n",
    "X = betas_tr\n",
    "Y = feats_tr\n",
    "X_te = betas_te_avg\n",
    "pipeline.fit(X, Y)\n",
    "scores = pipeline.predict(X_te)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(scores,feats_te_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e9e610-3c35-40d7-8fa7-4ce1f13f3ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18ef401d-08f5-46c9-bde6-f35596f78e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hdd/yuchen/anaconda3/envs/ldm/lib/python3.8/site-packages/nilearn/connectome/connectivity_matrices.py:509: FutureWarning: The default strategy for standardize is currently 'zscore' which incorrectly uses population std to calculate sample zscores. The new strategy 'zscore_sample' corrects this behavior by using the sample std. In release 0.13, the default strategy will be replaced by the new strategy and the 'zscore' option will be removed. Please use 'zscore_sample' instead.\n",
      "  covariances_std = [\n"
     ]
    }
   ],
   "source": [
    "num_roi = 4\n",
    "num_neurons = betas_tr_.shape[-1]\n",
    "num_pad = (num_neurons // num_roi + 1) * num_roi - num_neurons\n",
    "\n",
    "betas_tr = np.pad(betas_tr_, ((0, 0), (0, num_pad)), 'constant', constant_values=0)\n",
    "betas_tr_avg = np.pad(betas_tr_avg_, ((0, 0), (0, num_pad)), 'constant', constant_values=0)\n",
    "betas_te = np.pad(betas_te_, ((0, 0), (0, num_pad)), 'constant', constant_values=0)\n",
    "betas_te_avg = np.pad(betas_te_avg_, ((0, 0), (0, num_pad)), 'constant', constant_values=0)\n",
    "\n",
    "num_neurons_each_roi = int(betas_te_avg.shape[-1]/num_roi)\n",
    "conn_measure = connectome.ConnectivityMeasure(kind='correlation')\n",
    "function_con = betas_tr.reshape(-1, num_roi, num_neurons_each_roi)\n",
    "connectivity = conn_measure.fit_transform([np.mean(function_con, axis=-1)])\n",
    "for i in range(len(connectivity[0])):\n",
    "    connectivity[0][i, i] = 0  # Setting the i-th row to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a9b3a32-1b49-4b9d-93ca-a3bb7fd8bd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4)\n"
     ]
    }
   ],
   "source": [
    "adjmatrix = np.where(abs(connectivity[0]) > 0.8)\n",
    "edge_index = np.array(adjmatrix)  # Edge index matrix of shape [2, num_edges]\n",
    "print(edge_index.shape)\n",
    "\n",
    "edge_attr = []\n",
    "for idx in range(edge_index.shape[1]):\n",
    "    edge_attr.append(connectivity[0][edge_index[0,idx], edge_index[1,idx]])\n",
    "edge_attr = np.array(edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d65ddb67-a48d-4659-bb15-d1e72657c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trial_tr, num_trials_te = feats_tr.shape[0], feats_te_avg.shape[0]\n",
    "\n",
    "train_dataset = []\n",
    "for i in range(num_trial_tr):\n",
    "    train_dataset.append(Data(x=torch.tensor(betas_tr.reshape(-1, num_roi, num_neurons_each_roi)[i], dtype=torch.float), \n",
    "            edge_index=torch.tensor(edge_index, dtype=torch.long), \n",
    "            edge_attr=torch.tensor(edge_attr, dtype=torch.float),\n",
    "            y=torch.tensor(feats_tr[i], dtype=torch.float)\n",
    "            ))\n",
    "    \n",
    "test_dataset = []\n",
    "for i in range(num_trials_te):\n",
    "    test_dataset.append(Data(x=torch.tensor(betas_te_avg.reshape(-1, num_roi, num_neurons_each_roi)[i], dtype=torch.float), \n",
    "            edge_index=torch.tensor(edge_index, dtype=torch.long), \n",
    "            edge_attr=torch.tensor(edge_attr, dtype=torch.float),\n",
    "            y=torch.tensor(feats_te_avg[i], dtype=torch.float)\n",
    "            ))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dadecd5-48d4-4df6-b59f-8a393b6e8ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, TopKPooling, BatchNorm,ChebConv, SAGEConv,GATConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool,global_add_pool\n",
    "\n",
    "class GCNWithPooling(torch.nn.Module):\n",
    "    def __init__(self, num_neurons_each_roi):\n",
    "        super(GCNWithPooling, self).__init__()\n",
    "        \n",
    "        self.conv1 = ChebConv(num_neurons_each_roi, 256, K=4)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.conv = ChebConv(256, 256, K=3)\n",
    "        self.bn = BatchNorm(256)\n",
    "\n",
    "        nnconv_channel = 16\n",
    "        self.nnconv = nn.Sequential(\n",
    "            nn.Conv1d(1, nnconv_channel, kernel_size=1),\n",
    "            nn.ReLU(), \n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(nnconv_channel, nnconv_channel, kernel_size=3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.downsample = nn.Conv1d(nnconv_channel, 1, kernel_size=1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 6400)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch,edge_attr = data.x, data.edge_index, data.batch, data.edge_attr\n",
    "        batch_size = batch.max() + 1\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "\n",
    "        for i in range(1):\n",
    "            a = self.conv(x, edge_index, edge_attr)\n",
    "            a = self.bn(a)\n",
    "            x = x + F.relu(a)\n",
    "\n",
    "        x = global_mean_pool(x, batch) # print(x.shape) torch.Size([64, 256])\n",
    "\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.nnconv(x)\n",
    "        \n",
    "        for j in range(2):\n",
    "            x = x + self.net(x)\n",
    "        x = self.downsample(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2edb8c5d-afaa-40d5-ac55-50c16d2ab01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0] train loss 0.65145 val loss 0.62354\n",
      "[epoch 1] train loss 0.64557 val loss 0.62117\n",
      "[epoch 2] train loss 0.64482 val loss 0.62083\n",
      "[epoch 3] train loss 0.64456 val loss 0.62064\n",
      "[epoch 4] train loss 0.64436 val loss 0.62031\n",
      "[epoch 5] train loss 0.64402 val loss 0.61988\n",
      "[epoch 6] train loss 0.64311 val loss 0.61861\n",
      "[epoch 7] train loss 0.64154 val loss 0.6169\n",
      "[epoch 8] train loss 0.63933 val loss 0.61434\n",
      "[epoch 9] train loss 0.63718 val loss 0.61329\n",
      "[epoch 10] train loss 0.63523 val loss 0.61178\n",
      "[epoch 11] train loss 0.63353 val loss 0.61143\n",
      "[epoch 12] train loss 0.63188 val loss 0.61121\n",
      "[epoch 13] train loss 0.63019 val loss 0.61096\n",
      "[epoch 14] train loss 0.62867 val loss 0.61115\n",
      "[epoch 15] train loss 0.62739 val loss 0.611\n",
      "[epoch 16] train loss 0.62583 val loss 0.6112\n",
      "[epoch 17] train loss 0.62411 val loss 0.61157\n",
      "[epoch 18] train loss 0.6224 val loss 0.61154\n",
      "[epoch 19] train loss 0.62063 val loss 0.61303\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:1'\n",
    "model = GCNWithPooling(num_neurons_each_roi=num_neurons_each_roi).to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "num_epochs = 100\n",
    "patience = 5\n",
    "ct = 0\n",
    "best_loss_val = np.inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss_train = 0\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = batch.to(device)\n",
    "        batch_num = batch.batch.max() + 1\n",
    "        feats = batch['y'].reshape(batch_num,-1).to(device)\n",
    "        pred = model(batch)\n",
    "        \n",
    "        loss = loss_fn(pred, feats)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss_train += loss.item()\n",
    "    print('[epoch {}] train loss {}'.format(epoch, round(total_loss_train/len(train_dataloader), 5)), end = ' ')\n",
    "\n",
    "    model.eval()\n",
    "    total_loss_val = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            batch = batch.to(device)\n",
    "            batch_num = batch.batch.max() + 1\n",
    "            feats = batch['y'].reshape(batch_num,-1).to(device)\n",
    "            pred = model(batch)\n",
    "            loss = loss_fn(pred, feats)\n",
    "            total_loss_val += loss.item()\n",
    "        total_loss_val = total_loss_val/len(test_dataloader)\n",
    "        print('val loss {}'.format(round(total_loss_val, 5)))\n",
    "        \n",
    "        if total_loss_val < best_loss_val:\n",
    "            ct = 0\n",
    "            best_loss_val = total_loss_val\n",
    "            torch.save(model.state_dict(), '/hdd/yuchen/modelweights_visual_gcn_test.pth')\n",
    "        else:\n",
    "            ct += 1\n",
    "            if ct > patience:\n",
    "                print('end')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34532e39-6366-4904-bbda-f6f01911acd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5eba82-3818-4266-861c-a63600c17451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797ca880-1891-448e-b634-0e2457634bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a571a4-3b48-4639-92b9-a181b17d208c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4563000-839e-47b4-af2f-f217ff37c8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7318bf-595e-4f98-87a6-75dcf44c9e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbce9413-34a6-484e-a18c-3873f289ec19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "25344d33-c3d8-4f1e-8f64-d3fbcdace130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 6400])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(batch)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "243a89e2-b94b-4206-9144-4b90a2f765ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1480, 64])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(dataset[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "569d8a18-f8a6-4e42-95d1-cbe4b1950bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "bba313f1-d979-4e96-a0de-2bfbf53df258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1480, 64])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(batch)\n",
    "out.reshape(16, -1, 64).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a538617e-4f55-4fc5-9698-0464de17d274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=1)\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "934214e5-79cd-4af4-ba57-3df915ea4569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199989bd-ca74-4650-a787-c2cc286749a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512ff401-1d56-47cb-bc58-4925013398bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "06095492-27a1-4bca-ba7a-5e1ad767b259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "# Load a dataset\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "54158bca-c6d6-484d-973e-d02760dee2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "56bacd45-18fe-4cf2-8bd9-84919941d8c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GlobalStorage' object has no attribute 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\n",
      "File \u001b[0;32m/hdd/yuchen/anaconda3/envs/ldm/lib/python3.8/site-packages/torch_geometric/data/data.py:482\u001b[0m, in \u001b[0;36mData.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_store\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object was created by an older version of PyG. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf this error occurred while loading an already existing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset, remove the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directory in the dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot folder and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 482\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/hdd/yuchen/anaconda3/envs/ldm/lib/python3.8/site-packages/torch_geometric/data/storage.py:87\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GlobalStorage' object has no attribute 'num_classes'"
     ]
    }
   ],
   "source": [
    "data.num_classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
