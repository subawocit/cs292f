{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a961f16-7908-46a3-acf5-88464a7174cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hdd/yuchen/cs292/StableDiffusionReconstruction/codes/utils\n"
     ]
    }
   ],
   "source": [
    "%cd /hdd/yuchen/cs292/StableDiffusionReconstruction/codes/utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af41dd11-d565-46b3-9810-a563ff97129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nsd_access import NSDAccess\n",
    "import scipy.io\n",
    "import argparse, os\n",
    "import PIL\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm import tqdm\n",
    "from einops import repeat\n",
    "from torch import autocast\n",
    "from contextlib import nullcontext\n",
    "# from pytorch_lightning import seed_everything\n",
    "from nsd_access import NSDAccess\n",
    "from PIL import Image\n",
    "import scipy.io\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "665f97a4-d64e-42da-926e-b4149ad4154c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hdd/yuchen/cs292/StableDiffusionReconstruction/codes/diffusion_sd1/stable-diffusion\n"
     ]
    }
   ],
   "source": [
    "%cd /hdd/yuchen/cs292/StableDiffusionReconstruction/codes/diffusion_sd1/stable-diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14cc434b-17d1-41f6-a7b8-6b49141fd34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8f384c1-3d9a-49ab-b361-c3801ea7d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opt:\n",
    "    subject = 'subj01'\n",
    "    root_folder = '/hdd/yuchen/cs292/'\n",
    "    imgidx = (0, 10)\n",
    "    gpu = 1\n",
    "    seed = 42\n",
    "    featname = ''  # init_latent or c\n",
    "    use_stim = ''  # each or ave\n",
    "\n",
    "    target = ''  # init_latent or c\n",
    "    roi = ''  # ventral  or early \n",
    "    \n",
    "    method = 'cvpr'\n",
    "    \n",
    "opt = Opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "428d1b64-e3b6-4e24-904f-2e3b3aa98ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260dcaa4-9911-4008-8f21-47d343a3b853",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## make_subjmri.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6e938f5-0dc9-4979-9527-53212788be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = opt.subject\n",
    "# atlasname = 'streams'\n",
    "# atlasname = 'HCP_MMP1'\n",
    "atlasname = 'Kastner2015'\n",
    "\n",
    "    \n",
    "nsda = NSDAccess(opt.root_folder + 'nsd/')\n",
    "nsd_expdesign = scipy.io.loadmat(opt.root_folder + 'nsd/nsddata/experiments/nsd/nsd_expdesign.mat')\n",
    "\n",
    "    # Note that most of nsd_expdesign indices are 1-base index!\n",
    "    # This is why subtracting 1\n",
    "sharedix = nsd_expdesign['sharedix'] -1 \n",
    "\n",
    "behs = pd.DataFrame()\n",
    "for i in range(1,38):\n",
    "    beh = nsda.read_behavior(subject=subject, \n",
    "                                session_index=i)\n",
    "    behs = pd.concat((behs,beh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed0a8e74-0087-46d8-a498-2767bf55d662",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "stims_unique = behs['73KID'].unique() - 1\n",
    "stims_all = behs['73KID'] - 1\n",
    "\n",
    "savedir = f'{opt.root_folder}/mrifeat/{subject}/'\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(f'{savedir}/{subject}_stims.npy'):\n",
    "    np.save(f'{savedir}/{subject}_stims.npy',stims_all)\n",
    "    np.save(f'{savedir}/{subject}_stims_ave.npy',stims_unique)\n",
    "\n",
    "for i in range(1,38):\n",
    "    print(i)\n",
    "    beta_trial = nsda.read_betas(subject=subject, \n",
    "                                session_index=i, \n",
    "                                trial_index=[], # empty list as index means get all for this session\n",
    "                                data_type='betas_fithrf_GLMdenoise_RR',\n",
    "                                data_format='func1pt8mm')\n",
    "    if i==1:\n",
    "        betas_all = beta_trial\n",
    "    else:\n",
    "        betas_all = np.concatenate((betas_all,beta_trial),0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1ea8df9-0b61-4142-8012-19b501cbfacf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d5e7eb6-4474-4ff6-b93d-8a159905f73e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hdd/yuchen/cs292/nsd/nsddata/freesurfer/fsaverage/label/Kastner2015.mgz.ctab\n",
      "SKIP\n",
      "V1v (27750, 1229)\n",
      "(9841, 1229)\n",
      "V1d (27750, 1045)\n",
      "(9841, 1045)\n",
      "V2v (27750, 1054)\n",
      "(9841, 1054)\n",
      "V2d (27750, 628)\n",
      "(9841, 628)\n",
      "V3v (27750, 763)\n",
      "(9841, 763)\n",
      "V3d (27750, 687)\n",
      "(9841, 687)\n",
      "hV4 (27750, 473)\n",
      "(9841, 473)\n",
      "VO1 (27750, 216)\n",
      "(9841, 216)\n",
      "VO2 (27750, 313)\n",
      "(9841, 313)\n",
      "PHC1 (27750, 199)\n",
      "(9841, 199)\n",
      "PHC2 (27750, 183)\n",
      "(9841, 183)\n",
      "TO2 (27750, 55)\n",
      "(9841, 55)\n",
      "TO1 (27750, 284)\n",
      "(9841, 284)\n",
      "LO2 (27750, 193)\n",
      "(9841, 193)\n",
      "LO1 (27750, 343)\n",
      "(9841, 343)\n",
      "V3B (27750, 315)\n",
      "(9841, 315)\n",
      "V3A (27750, 696)\n",
      "(9841, 696)\n",
      "IPS0 (27750, 606)\n",
      "(9841, 606)\n",
      "IPS1 (27750, 462)\n",
      "(9841, 462)\n",
      "IPS2 (27750, 500)\n",
      "(9841, 500)\n",
      "IPS3 (27750, 508)\n",
      "(9841, 508)\n",
      "IPS4 (27750, 65)\n",
      "(9841, 65)\n",
      "IPS5 (27750, 14)\n",
      "(9841, 14)\n",
      "SPL1 (27750, 164)\n",
      "(9841, 164)\n",
      "FEF (27750, 72)\n",
      "(9841, 72)\n"
     ]
    }
   ],
   "source": [
    "atlasname = 'Kastner2015'\n",
    "nsda = NSDAccess(opt.root_folder + 'nsd/')\n",
    "\n",
    "atlas = nsda.read_atlas_results(subject=subject, atlas=atlasname, data_format='func1pt8mm')\n",
    "\n",
    "for roi,val in atlas[1].items():\n",
    "    if val == 0:\n",
    "        print('SKIP')\n",
    "        continue\n",
    "    else:\n",
    "        betas_roi = betas_all[:,atlas[0].transpose([2,1,0])==val]\n",
    "\n",
    "    print(roi, betas_roi.shape)\n",
    "        \n",
    "    # Averaging for each stimulus\n",
    "    betas_roi_ave = []\n",
    "    for stim in stims_unique:\n",
    "        stim_mean = np.mean(betas_roi[stims_all == stim,:],axis=0)\n",
    "        betas_roi_ave.append(stim_mean)\n",
    "    betas_roi_ave = np.stack(betas_roi_ave)\n",
    "    print(betas_roi_ave.shape)\n",
    "        \n",
    "        # Train/Test Split\n",
    "        # ALLDATA\n",
    "    betas_tr = []\n",
    "    betas_te = []\n",
    "\n",
    "    for idx,stim in enumerate(stims_all):\n",
    "        if stim in sharedix:\n",
    "            betas_te.append(betas_roi[idx,:])\n",
    "        else:\n",
    "            betas_tr.append(betas_roi[idx,:])\n",
    "\n",
    "    betas_tr = np.stack(betas_tr)\n",
    "    betas_te = np.stack(betas_te)    \n",
    "        \n",
    "        # AVERAGED DATA        \n",
    "    betas_ave_tr = []\n",
    "    betas_ave_te = []\n",
    "    for idx,stim in enumerate(stims_unique):\n",
    "        if stim in sharedix:\n",
    "            betas_ave_te.append(betas_roi_ave[idx,:])\n",
    "        else:\n",
    "            betas_ave_tr.append(betas_roi_ave[idx,:])\n",
    "    betas_ave_tr = np.stack(betas_ave_tr)\n",
    "    betas_ave_te = np.stack(betas_ave_te)    \n",
    "        \n",
    "        # Save\n",
    "    np.save(f'{savedir}/{subject}_{roi}_betas_tr.npy',betas_tr)\n",
    "    np.save(f'{savedir}/{subject}_{roi}_betas_te.npy',betas_te)\n",
    "    np.save(f'{savedir}/{subject}_{roi}_betas_ave_tr.npy',betas_ave_tr)\n",
    "    np.save(f'{savedir}/{subject}_{roi}_betas_ave_te.npy',betas_ave_te)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66ebaac1-6b0a-4053-9b97-a9af6212f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'subj01'\n",
    "roi = 'V1'\n",
    "ct = 0\n",
    "for roi in ['V1', 'V2', 'V3', 'V4']:\n",
    "    savedir = f'{opt.root_folder}/mrifeat/{subject}/'\n",
    "    ct += np.load(f'{savedir}/{subject}_{roi}_betas_tr.npy').shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7d19d5e-e932-4802-a79d-3f08f7a5d808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10631"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49994578-f97e-4277-9207-2f03619efaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24980, 5917)\n",
      "(24980, 7604)\n"
     ]
    }
   ],
   "source": [
    "print(np.load(f'{savedir}/{subject}_early_betas_tr.npy').shape)\n",
    "print(np.load(f'{savedir}/{subject}_ventral_betas_tr.npy').shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0b951d-f3a2-4e7e-8608-4d717be2c643",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## img2feat_sd1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73b03d80-97b1-4ca1-9e08-24392211ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_config(config, ckpt, gpu, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "    model.cuda(f\"cuda:{gpu}\")\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_img_from_arr(img_arr,resolution):\n",
    "    image = Image.fromarray(img_arr).convert(\"RGB\")\n",
    "    w, h = resolution, resolution\n",
    "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2.*image - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fa5c857-65d2-46af-8304-b57ab615ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opt:\n",
    "    subject = 'subj01'\n",
    "    root_folder = '/hdd/yuchen/'\n",
    "    imgidx = (0, 73000)\n",
    "    gpu = 1\n",
    "    seed = 42\n",
    "    \n",
    "opt = Opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25312b57-270e-4d82-b7e2-77570d707e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_everything(opt.seed)\n",
    "imgidx = opt.imgidx\n",
    "gpu = opt.gpu\n",
    "resolution = 320\n",
    "batch_size = 1\n",
    "ddim_steps = 50\n",
    "ddim_eta = 0.0\n",
    "strength = 0.8\n",
    "scale = 5.0\n",
    "nsda = NSDAccess(opt.root_folder + 'nsd/')\n",
    "config = opt.root_folder + 'StableDiffusionReconstruction/codes/diffusion_sd1/stable-diffusion/configs/stable-diffusion/v1-inference.yaml'\n",
    "ckpt = opt.root_folder + 'StableDiffusionReconstruction/codes/diffusion_sd1/stable-diffusion/models/ldm/stable-diffusion-v1/sd-v1-4.ckpt'\n",
    "config = OmegaConf.load(f\"{config}\")\n",
    "torch.cuda.set_device(gpu)\n",
    "os.makedirs(f'{opt.root_folder}nsdfeat/init_latent/', exist_ok=True)\n",
    "os.makedirs(f'{opt.root_folder}nsdfeat/c/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "239d5cbc-3eff-4d58-b14d-41a7035c4739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /hdd/yuchen/StableDiffusionReconstruction/codes/diffusion_sd1/stable-diffusion/models/ldm/stable-diffusion-v1/sd-v1-4.ckpt\n",
      "Global Step: 470000\n",
      "LatentDiffusion: Running in eps-prediction mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hdd/yuchen/anaconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:258: LightningDeprecationWarning: `pytorch_lightning.utilities.distributed.rank_zero_only` has been deprecated in v1.8.1 and will be removed in v1.10.0. You can import it from `pytorch_lightning.utilities` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'logit_scale', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'visual_projection.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target t_enc is 40 steps\n"
     ]
    }
   ],
   "source": [
    "# Load moodels\n",
    "precision = 'autocast'\n",
    "precision_scope = autocast if precision == \"autocast\" else nullcontext\n",
    "model = load_model_from_config(config, f\"{ckpt}\", gpu)\n",
    "device = torch.device(f\"cuda:{gpu}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "sampler = DDIMSampler(model)\n",
    "sampler.make_schedule(ddim_num_steps=ddim_steps, ddim_eta=ddim_eta, verbose=False)\n",
    "assert 0. <= strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "t_enc = int(strength * ddim_steps)\n",
    "print(f\"target t_enc is {t_enc} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d192f0b8-5a59-4b1b-bfb1-69b1e42e72db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.9335, device='cuda:1'), tensor(-3.5237, device='cuda:1'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_latent.max(), init_latent.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "142e6cf7-6be1-4c7d-85d9-5d14bb1a26f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/73000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing image 000000\n",
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/73000 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Sample\n",
    "for s in tqdm(range(imgidx[0],imgidx[1])):\n",
    "    print(f\"Now processing image {s:06}\")\n",
    "    prompt = []\n",
    "    prompts = nsda.read_image_coco_info([s],info_type='captions')\n",
    "    for p in prompts:\n",
    "        prompt.append(p['caption'])    \n",
    "        \n",
    "    img = nsda.read_images(s)\n",
    "    init_image = load_img_from_arr(img,resolution).to(device)\n",
    "    init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
    "    init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with precision_scope(\"cuda\"):\n",
    "            with model.ema_scope():\n",
    "                uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                c = model.get_learned_conditioning(prompt).mean(axis=0).unsqueeze(0)\n",
    "    \n",
    "                        # encode (scaled latent)\n",
    "                z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
    "                        # decode it\n",
    "                samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=scale,\n",
    "                                                unconditional_conditioning=uc,)\n",
    "                        \n",
    "    init_latent = init_latent.cpu().detach().numpy().flatten()\n",
    "    c = c.cpu().detach().numpy().flatten()\n",
    "    np.save(f'{opt.root_folder}nsdfeat/init_latent/{s:06}.npy',init_latent)\n",
    "    np.save(f'{opt.root_folder}nsdfeat/c/{s:06}.npy',c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf170989-a00c-4a19-8bb3-04e8b48ee264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc1a83-dc8c-418c-a749-96b49681f419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a138d419-915c-4761-ae0e-fad82f1c958b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## make_subjstim.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cb44e99-773b-4bf5-baa4-895095e93530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subj01'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb4d93a2-496a-44b0-964a-47d4f7c756cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.featname = 'init_latent'\n",
    "opt.use_stim = 'each'\n",
    "\n",
    "subject= opt.subject\n",
    "use_stim = opt.use_stim\n",
    "featname = opt.featname\n",
    "topdir = opt.root_folder + 'nsdfeat/'\n",
    "savedir = f'{topdir}/subjfeat/'\n",
    "featdir = f'{topdir}/{featname}/'\n",
    "\n",
    "nsd_expdesign = scipy.io.loadmat(opt.root_folder + 'nsd/nsddata/experiments/nsd/nsd_expdesign.mat')\n",
    "\n",
    "    # Note that most of them are 1-base index!\n",
    "    # This is why I subtract 1\n",
    "sharedix = nsd_expdesign['sharedix'] -1 \n",
    "\n",
    "if use_stim == 'ave':\n",
    "    stims = np.load(f'{opt.root_folder}mrifeat/{subject}/{subject}_stims_ave.npy')\n",
    "else: # Each\n",
    "    stims = np.load(f'{opt.root_folder}mrifeat/{subject}/{subject}_stims.npy')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e24ea496-e913-4e09-9e04-26b252aacfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27750it [00:05, 4880.60it/s]\n"
     ]
    }
   ],
   "source": [
    "feats = []\n",
    "tr_idx = np.zeros(len(stims))\n",
    "\n",
    "for idx, s in tqdm(enumerate(stims)): \n",
    "    if s in sharedix:\n",
    "        tr_idx[idx] = 0\n",
    "    else:\n",
    "        tr_idx[idx] = 1    \n",
    "    feat = np.load(f'{featdir}/{s:06}.npy')\n",
    "    feats.append(feat)\n",
    "\n",
    "feats = np.stack(feats)    \n",
    "\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "feats_tr = feats[tr_idx==1,:]\n",
    "feats_te = feats[tr_idx==0,:]\n",
    "np.save(f'{opt.root_folder}mrifeat/{subject}/{subject}_stims_tridx.npy',tr_idx)\n",
    "np.save(f'{savedir}/{subject}_{use_stim}_{featname}_tr.npy',feats_tr)\n",
    "np.save(f'{savedir}/{subject}_{use_stim}_{featname}_te.npy',feats_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ca424a-7b49-4526-8712-8601ed3f2050",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ridge.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d807a38-fd24-4664-ab7f-503ed3dd194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os\n",
    "import numpy as np\n",
    "from himalaya.backend import set_backend\n",
    "from himalaya.ridge import RidgeCV\n",
    "from himalaya.scoring import correlation_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3702317-95ed-44f5-9872-17b1f5efc93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opt:\n",
    "    subject = 'subj01'\n",
    "    root_folder = '/hdd/yuchen/'\n",
    "    imgidx = (0, 73000)\n",
    "    gpu = 1\n",
    "    seed = 42\n",
    "    featname = ''  # init_latent or c\n",
    "    use_stim = ''  # each or ave\n",
    "\n",
    "    target = ''  # init_latent or c\n",
    "    roi = ''  # ventral  or early   TODO: how do they select that ?\n",
    "    \n",
    "opt = Opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e439f87d-9eaf-4ec3-be87-8c22915dd3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.target = 'init_latent'\n",
    "opt.roi = 'early'\n",
    "\n",
    "target = opt.target\n",
    "roi = opt.roi\n",
    "\n",
    "backend = set_backend(\"numpy\", on_error=\"warn\")\n",
    "subject=opt.subject\n",
    "\n",
    "if target == 'c' or target == 'init_latent': # CVPR\n",
    "    alpha = [0.000001,0.00001,0.0001,0.001,0.01, 0.1, 1]\n",
    "else: # text / GAN / depth decoding (with much larger number of voxels)\n",
    "    alpha = [10000, 20000, 40000]\n",
    "\n",
    "ridge = RidgeCV(alphas=alpha)\n",
    "\n",
    "preprocess_pipeline = make_pipeline(\n",
    "        StandardScaler(with_mean=True, with_std=True),\n",
    "    )\n",
    "pipeline = make_pipeline(\n",
    "        preprocess_pipeline,\n",
    "        ridge,\n",
    ")    \n",
    "mridir = f'{opt.root_folder}mrifeat/{subject}'\n",
    "featdir = f'{opt.root_folder}nsdfeat/subjfeat'\n",
    "savedir = f'{opt.root_folder}decoded/{subject}/'\n",
    "os.makedirs(savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d37a91d-f5ea-4715-a66a-638114433daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now making decoding model for... subj01:  early, init_latent\n",
      "X (24980, 5917), Y (24980, 6400), X_te (982, 5917), Y_te (982, 6400)\n",
      "Prediction accuracy is: 0.239\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "X_te = []\n",
    "for croi in [roi]:\n",
    "    if 'conv' in target: # We use averaged features for GAN due to large number of dimension of features\n",
    "        cX = np.load(f'{mridir}/{subject}_{croi}_betas_ave_tr.npy').astype(\"float32\")\n",
    "    else:\n",
    "        cX = np.load(f'{mridir}/{subject}_{croi}_betas_tr.npy').astype(\"float32\")\n",
    "    cX_te = np.load(f'{mridir}/{subject}_{croi}_betas_ave_te.npy').astype(\"float32\")\n",
    "    X.append(cX)\n",
    "    X_te.append(cX_te)\n",
    "X = np.hstack(X)\n",
    "X_te = np.hstack(X_te)\n",
    "    \n",
    "Y = np.load(f'{featdir}/{subject}_each_{target}_tr.npy').astype(\"float32\").reshape([X.shape[0],-1])\n",
    "Y_te = np.load(f'{featdir}/{subject}_ave_{target}_te.npy').astype(\"float32\").reshape([X_te.shape[0],-1])\n",
    "    \n",
    "print(f'Now making decoding model for... {subject}:  {roi}, {target}')\n",
    "print(f'X {X.shape}, Y {Y.shape}, X_te {X_te.shape}, Y_te {Y_te.shape}')\n",
    "pipeline.fit(X, Y)\n",
    "scores = pipeline.predict(X_te)\n",
    "rs = correlation_score(Y_te.T,scores.T)\n",
    "print(f'Prediction accuracy is: {np.mean(rs):3.3}')\n",
    "\n",
    "np.save(f'{savedir}/{subject}_{\"_\".join(roi)}_scores_{target}.npy',scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bad971d6-8a0e-4f98-837e-f64e1e7e3686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6907157"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target  = 'c'\n",
    "subject = 'subj01'\n",
    "featdir = '/hdd/yuchen/nsdfeat/subjfeat'\n",
    "Y_te = np.load(f'{featdir}/{subject}_ave_{target}_te.npy').astype(\"float32\")\n",
    "scores = np.load('/hdd/yuchen/decoded/subj01/subj01_v_e_n_t_r_a_l_scores_c.npy')\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(Y_te,scores)\n",
    "# mse for text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34fb17fa-6862-483a-ab6e-efcfbab50e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61325544"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(Y_te,scores)\n",
    "# mse for image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e97da3-52cc-45de-8b0a-d3f790f915da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c750881d-bc75-43cd-900a-2ed16760f38a",
   "metadata": {},
   "source": [
    "## diffusion_decoding.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da054ec7-c9be-48fd-bd53-052bdc7d3c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from PIL import Image\n",
    "import scipy.io\n",
    "import argparse, os\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm import trange\n",
    "from einops import rearrange\n",
    "from torch import autocast\n",
    "from contextlib import nullcontext\n",
    "# from pytorch_lightning import seed_everything\n",
    "import sys\n",
    "sys.path.append(\"../utils/\")\n",
    "from nsd_access.nsda import NSDAccess\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6151b33-0081-46e5-949b-73adc4fb8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_config(config, ckpt, gpu, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "    model.cuda(f\"cuda:{gpu}\")\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_img_from_arr(img_arr):\n",
    "    image = Image.fromarray(img_arr).convert(\"RGB\")\n",
    "    w, h = 512, 512\n",
    "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2.*image - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a88ea37-04ab-4430-91e8-0012b549b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opt:\n",
    "    subject = 'subj01'\n",
    "    root_folder = '/hdd/yuchen/cs292/'\n",
    "    imgidx = (0, 10)\n",
    "    gpu = 1\n",
    "    seed = 42\n",
    "    featname = ''  # init_latent or c\n",
    "    use_stim = ''  # each or ave\n",
    "\n",
    "    target = ''  # init_latent or c\n",
    "    roi = ''  # ventral  or early \n",
    "    \n",
    "    method = 'cvpr'\n",
    "    \n",
    "opt = Opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f68b1ff1-ae45-4b84-9cd9-cdac5621e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgidx = opt.imgidx\n",
    "gpu = opt.gpu\n",
    "method = opt.method\n",
    "subject=opt.subject\n",
    "torch.cuda.set_device(opt.gpu)\n",
    "\n",
    "captdir = f'{opt.root_folder}decoded/{subject}/captions/'\n",
    "\n",
    "    # Load NSD information\n",
    "nsd_expdesign = scipy.io.loadmat(f'{opt.root_folder}nsd/nsddata/experiments/nsd/nsd_expdesign.mat')\n",
    "\n",
    "    # Note that mos of them are 1-base index!\n",
    "    # This is why I subtract 1\n",
    "sharedix = nsd_expdesign['sharedix'] -1 \n",
    "\n",
    "nsda = NSDAccess(f'{opt.root_folder}nsd/')\n",
    "sf = h5py.File(nsda.stimuli_file, 'r')\n",
    "sdataset = sf.get('imgBrick')\n",
    "\n",
    "stims_ave = np.load(f'{opt.root_folder}mrifeat/{subject}/{subject}_stims_ave.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23683c66-a7ac-4575-8750-9f7318e45272",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_idx = np.zeros_like(stims_ave)\n",
    "for idx, s in enumerate(stims_ave):\n",
    "    if s in sharedix:\n",
    "        tr_idx[idx] = 0\n",
    "    else:\n",
    "        tr_idx[idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d69bac8-ae9e-49da-a106-2b0b0ac0db68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /hdd/yuchen/cs292/StableDiffusionReconstruction/codes/diffusion_sd1/stable-diffusion/models/ldm/stable-diffusion-v1/sd-v1-4.ckpt\n",
      "Global Step: 470000\n",
      "LatentDiffusion: Running in eps-prediction mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hdd/yuchen/anaconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:258: LightningDeprecationWarning: `pytorch_lightning.utilities.distributed.rank_zero_only` has been deprecated in v1.8.1 and will be removed in v1.10.0. You can import it from `pytorch_lightning.utilities` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.pre_layrnorm.bias', 'visual_projection.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'logit_scale', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "    # Load Stable Diffusion Model\n",
    "path = '/hdd/yuchen/cs292/StableDiffusionReconstruction/codes/diffusion_sd1'\n",
    "config = path + '/stable-diffusion/configs/stable-diffusion/v1-inference.yaml'\n",
    "ckpt = path + '/stable-diffusion/models/ldm/stable-diffusion-v1/sd-v1-4.ckpt'\n",
    "config = OmegaConf.load(f\"{config}\")\n",
    "torch.cuda.set_device(gpu)\n",
    "model = load_model_from_config(config, f\"{ckpt}\", gpu)\n",
    "\n",
    "n_samples = 1\n",
    "ddim_steps = 50\n",
    "ddim_eta = 0.0\n",
    "strength = 0.8\n",
    "scale = 5.0\n",
    "n_iter = 5\n",
    "precision = 'autocast'\n",
    "precision_scope = autocast if precision == \"autocast\" else nullcontext\n",
    "batch_size = n_samples\n",
    "device = torch.device(f\"cuda:{opt.gpu}\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "022ccde2-772c-489c-964e-122184dcb62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target t_enc is 40 steps\n"
     ]
    }
   ],
   "source": [
    "outdir = f'{opt.root_folder}decoded/image-{method}/{subject}/'\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "sample_path = os.path.join(outdir, f\"samples\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "precision = 'autocast'\n",
    "device = torch.device(f\"cuda:{gpu}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "sampler = DDIMSampler(model)\n",
    "\n",
    "sampler.make_schedule(ddim_num_steps=ddim_steps, ddim_eta=ddim_eta, verbose=False)\n",
    "\n",
    "assert 0. <= strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "t_enc = int(strength * ddim_steps)\n",
    "print(f\"target t_enc is {t_enc} steps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e874c-e96c-45a5-a0bf-e3c1bf0dadbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "scores_c = np.load('/hdd/yuchen/cs292/temp_lst_latent_c_roi.npy')\n",
    "scores_latent = np.load('/hdd/yuchen/cs292/temp_lst_latent_init_roi.npy')\n",
    "sample_path = '/hdd/yuchen/cs292/decoded/image-cvpr/subj01/nsd_roi/samples/'\n",
    "\n",
    "imgidx_te = np.where(tr_idx==0)[0]\n",
    "# ct = 0\n",
    "num_test_img = imgidx_te.shape[0]\n",
    "\n",
    "for imgidx in range(num_test_img):\n",
    "        \n",
    "# Load z (Image)\n",
    "    imgidx_te = np.where(tr_idx==0)[0][imgidx] # Extract test image index\n",
    "    idx73k= stims_ave[imgidx_te]\n",
    "    Image.fromarray(np.squeeze(sdataset[idx73k,:,:,:]).astype(np.uint8)).save(\n",
    "        os.path.join(sample_path, f\"{imgidx:05}_org.png\"))    \n",
    "        \n",
    "    if method in ['cvpr','text']:\n",
    "        # roi_latent = 'early'\n",
    "        # roi_latent = 'e_a_r_l_y'\n",
    "        \n",
    "        # scores_latent = np.load(f'{opt.root_folder}decoded/{subject}/{subject}_{roi_latent}_scores_init_latent.npy')\n",
    "        imgarr = torch.Tensor(scores_latent[imgidx,:].reshape(4,40,40)).unsqueeze(0).to(f'cuda:{opt.gpu}')\n",
    "    \n",
    "        # Generate image from Z\n",
    "        precision_scope = autocast if precision == \"autocast\" else nullcontext\n",
    "        with torch.no_grad():\n",
    "            with precision_scope(\"cuda\"):\n",
    "                with model.ema_scope():\n",
    "                    x_samples = model.decode_first_stage(imgarr)\n",
    "                    x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "    \n",
    "                    for x_sample in x_samples:\n",
    "                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "        im = Image.fromarray(x_sample.astype(np.uint8)).resize((512,512))\n",
    "        im = np.array(im)\n",
    "    \n",
    "    init_image = load_img_from_arr(im).to(f'cuda:{opt.gpu}')\n",
    "    init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
    "    \n",
    "    # Load c (Semantics)\n",
    "    if method == 'cvpr':\n",
    "        # roi_c = 'ventral'\n",
    "        # roi_c = 'v_e_n_t_r_a_l'\n",
    "        # scores_c = np.load(f'{opt.root_folder}decoded/{subject}/{subject}_{roi_c}_scores_c.npy')\n",
    "        # carr = scores_c[imgidx,:].reshape(77,768)\n",
    "        carr = scores_c[imgidx,:]\n",
    "        c = torch.Tensor(carr).unsqueeze(0).to(f'cuda:{opt.gpu}')\n",
    "    \n",
    "        # Generate image from Z (image) + C (semantics)\n",
    "    base_count = 0\n",
    "    with torch.no_grad():\n",
    "        with precision_scope(\"cuda\"):\n",
    "            with model.ema_scope():\n",
    "                for n in trange(n_iter, desc=\"Sampling\"):\n",
    "                    uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "    \n",
    "                        # encode (scaled latent)\n",
    "                    z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
    "                        # decode it\n",
    "                    samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=scale,\n",
    "                                                unconditional_conditioning=uc,)\n",
    "    \n",
    "                    x_samples = model.decode_first_stage(samples)\n",
    "                    x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "    \n",
    "                    for x_sample in x_samples:\n",
    "                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                    Image.fromarray(x_sample.astype(np.uint8)).save(\n",
    "                        os.path.join(sample_path, f\"{imgidx:05}_{base_count:03}.png\"))    \n",
    "                    base_count += 1\n",
    "    # if imgidx == 100:\n",
    "    #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b8fa2f-9828-4819-a5fc-b449ad8fdc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pkill tmux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8ab0b-5f61-4086-8912-538a799aa87b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
