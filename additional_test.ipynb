{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d548004-b9c2-4b88-97e4-4a168c060f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hdd/yuchen/StableDiffusionReconstruction/codes/utils\n"
     ]
    }
   ],
   "source": [
    "%cd /hdd/yuchen/StableDiffusionReconstruction/codes/utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63dbeae0-3ab2-4a4a-af7a-815226ed2a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nsd_access import NSDAccess\n",
    "import scipy.io\n",
    "import argparse, os\n",
    "import PIL\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm import tqdm\n",
    "from einops import repeat\n",
    "from torch import autocast\n",
    "from contextlib import nullcontext\n",
    "# from pytorch_lightning import seed_everything\n",
    "from nsd_access import NSDAccess\n",
    "from PIL import Image\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "import scipy.io\n",
    "from einops import rearrange\n",
    "import random\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "from skimage.metrics import structural_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a39f732-3dc9-4da1-a03d-a97c9de31596",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3753614c-9e21-46e1-9db5-fcf6908e363c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea4241bf-b00b-44cc-ab2d-f3fab82a09b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_one_set(i, lpips, folder_path):\n",
    "    with torch.no_grad():\n",
    "        base = str(f'{i:05}')\n",
    "        orig = folder_path + f'{base}_org.png'\n",
    "        orig_ = Image.open(orig).resize((512,512), Image.Resampling.BILINEAR)\n",
    "        img2 = np.array(orig_)\n",
    "        \n",
    "        psm, ssim, pcc = 0, 0, 0\n",
    "        for j in range(4):\n",
    "            img = folder_path + f'{base}_{j:03}.png'\n",
    "            img_ = Image.open(img)\n",
    "            img1 = np.array(img_)\n",
    "            \n",
    "            if img1.shape[-1] == 3: img1 = rearrange(img1, 'w h c -> c w h')\n",
    "            if img2.shape[-1] == 3: img2 = rearrange(img2, 'w h c -> c w h')\n",
    "            if img1.max() > 1: img1 = img1 / 255.0\n",
    "            if img2.max() > 1: img2 = img2 / 255.0\n",
    "            if len(img1.shape) == 3: img1 = np.expand_dims(img1, axis=0)\n",
    "            if len(img2.shape) == 3: img2 = np.expand_dims(img2, axis=0)\n",
    "                \n",
    "            psm += lpips(torch.FloatTensor(img1).to(device), torch.FloatTensor(img2).to(device)).item()\n",
    "            ssim += structural_similarity(img1.squeeze(),img2.squeeze(), data_range=1, channel_axis=0)\n",
    "            pcc += np.corrcoef(img1.reshape(-1), img2.reshape(-1))[0, 1]\n",
    "            \n",
    "    return psm/4, ssim/4, pcc/4\n",
    "\n",
    "folder_path = '/hdd/yuchen/decoded/image-cvpr/subj01/samples/'\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "lpips = LearnedPerceptualImagePatchSimilarity(net_type='alex', normalize=True).to(device)\n",
    "\n",
    "score = {'psm': [], 'ssim': [], 'pcc': []}\n",
    "for i in range(30):\n",
    "    test_score = compare_one_set(i, lpips, folder_path)\n",
    "    score['psm'].append(test_score[0])\n",
    "    score['ssim'].append(test_score[1])\n",
    "    score['pcc'].append(test_score[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "268e1979-f53f-4a16-b113-697be0a8852a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6775105200707913, 0.2232460986445165, 0.27371258050492725)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(score['psm']), np.mean(score['ssim']), np.mean(score['pcc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e01cba2-abc1-4c30-9731-de1059fdeea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "694e990a-0d15-46cd-8f2f-c9d4b8fccdf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "win_size exceeds image extent. Either ensure that your images are at least 7x7; or pass win_size explicitly in the function call, with an odd value less than or equal to the smaller side of your images. If your images are multichannel (with color channels), set channel_axis to the axis number corresponding to the channels.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m img1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m3\u001b[39m))\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      5\u001b[0m img2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m3\u001b[39m))\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m----> 7\u001b[0m ssim_score \u001b[38;5;241m=\u001b[39m \u001b[43mstructural_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultichannel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#score: 0.0018769083894301646\u001b[39;00m\n",
      "File \u001b[0;32m/hdd/yuchen/anaconda3/envs/ldm/lib/python3.8/site-packages/skimage/metrics/_structural_similarity.py:178\u001b[0m, in \u001b[0;36mstructural_similarity\u001b[0;34m(im1, im2, win_size, gradient, data_range, channel_axis, gaussian_weights, full, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m         win_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m   \u001b[38;5;66;03m# backwards compatibility\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many((np\u001b[38;5;241m.\u001b[39masarray(im1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m-\u001b[39m win_size) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwin_size exceeds image extent. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEither ensure that your images are \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat least 7x7; or pass win_size explicitly \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min the function call, with an odd value \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mless than or equal to the smaller side of your \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages. If your images are multichannel \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(with color channels), set channel_axis to \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe axis number corresponding to the channels.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (win_size \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWindow size must be odd.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: win_size exceeds image extent. Either ensure that your images are at least 7x7; or pass win_size explicitly in the function call, with an odd value less than or equal to the smaller side of your images. If your images are multichannel (with color channels), set channel_axis to the axis number corresponding to the channels."
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "from skimage.metrics import structural_similarity\n",
    "\n",
    "img1 = np.random.randint(0, 255, size=(200, 200, 3)).astype(np.float32)\n",
    "img2 = np.random.randint(0, 255, size=(200, 200, 3)).astype(np.float32)\n",
    "\n",
    "ssim_score = structural_similarity(img1, img2, multichannel=True) #score: 0.0018769083894301646\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82714ee0-4184-4bdc-ab7c-71418b009227",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mscore\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'score' is not defined"
     ]
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f8053352-b6e0-4be4-9d8f-c8978a3405ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = np.array(orig_)\n",
    "img2 = np.array(img_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "153c175f-49c4-4fd7-8ee0-419e501df7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = np.array(img_)\n",
    "img2 = np.array(orig_.resize((img1.shape[0], img1.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b7abf8-2004-4947-a8b9-f8a18b9554dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "03ff11ac-89de-4ce5-a322-f34235cc33f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = np.array(img_)\n",
    "img2 = np.array(orig_.resize((img1.shape[0], img1.shape[1])))\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "lpips = LearnedPerceptualImagePatchSimilarity(net_type='alex', normalize=True).to(device)\n",
    "\n",
    "# @torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    if img1.shape[-1] == 3:\n",
    "        img1 = rearrange(img1, 'w h c -> c w h')\n",
    "        img2 = rearrange(img2, 'w h c -> c w h')\n",
    "    img1 = img1 / 255.0\n",
    "    img2 = img2 / 255.0\n",
    "    img1 = np.expand_dims(img1, axis=0)\n",
    "    img2 = np.expand_dims(img2, axis=0)\n",
    "    t = lpips(torch.FloatTensor(img1).to(device), torch.FloatTensor(img2).to(device)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e8dc4a56-bbd5-4c2a-a471-020898b9493d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c3d2368-e287-473f-99bc-e7d06bd8ba71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LearnedPerceptualImagePatchSimilarity(\n",
       "  (net): NoTrainLpips(\n",
       "    (scaling_layer): ScalingLayer()\n",
       "    (net): alexnet(\n",
       "      (slice1): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (slice2): Sequential(\n",
       "        (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "        (4): ReLU(inplace=True)\n",
       "      )\n",
       "      (slice3): Sequential(\n",
       "        (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "      (slice4): Sequential(\n",
       "        (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (9): ReLU(inplace=True)\n",
       "      )\n",
       "      (slice5): Sequential(\n",
       "        (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (11): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (lin0): NetLinLayer(\n",
       "      (model): Sequential(\n",
       "        (0): Dropout(p=0.5, inplace=False)\n",
       "        (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (lin1): NetLinLayer(\n",
       "      (model): Sequential(\n",
       "        (0): Dropout(p=0.5, inplace=False)\n",
       "        (1): Conv2d(192, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (lin2): NetLinLayer(\n",
       "      (model): Sequential(\n",
       "        (0): Dropout(p=0.5, inplace=False)\n",
       "        (1): Conv2d(384, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (lin3): NetLinLayer(\n",
       "      (model): Sequential(\n",
       "        (0): Dropout(p=0.5, inplace=False)\n",
       "        (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (lin4): NetLinLayer(\n",
       "      (model): Sequential(\n",
       "        (0): Dropout(p=0.5, inplace=False)\n",
       "        (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (lins): ModuleList(\n",
       "      (0): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(192, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (2): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(384, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (4): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LearnedPerceptualImagePatchSimilarity(net_type='alex').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801c845e-2c04-42f0-bf67-116463fae998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c379824f-e3f5-4ebb-a25c-b77b2c2ff90e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4806de-13d3-48d1-a280-2479a39b55ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5583710-7157-4b01-878b-7c98c5f20070",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = scipy.io.loadmat( '/hdd/yuchen/nsd/nsddata/experiments/nsd/nsd_expdesign.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0585ff16-5db3-446e-abe7-6552130d67f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'basiccnt', 'masterordering', 'sharedix', 'stimpattern', 'subjectim'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dc8f33b-1a1d-435d-b60d-c8edb3036361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2951,  2991,  3050, ..., 19472, 49508, 11124],\n",
       "       [ 2951,  2991,  3050, ..., 29015, 40443,  7566],\n",
       "       [ 2951,  2991,  3050, ..., 40627, 60549, 60491],\n",
       "       ...,\n",
       "       [ 2951,  2991,  3050, ..., 22828, 64982, 39213],\n",
       "       [ 2951,  2991,  3050, ..., 68368, 68973, 16416],\n",
       "       [ 2951,  2991,  3050, ..., 24254, 14948, 42093]], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['subjectim']  #  1-base index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09025d9f-6090-4705-b4b8-87547751d80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(test['subjectim'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe5c6512-d44d-4547-8f79-5d0e4d610341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([1,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "194266e0-24e7-42ad-a9b5-d10d0603e69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2951,  2991,  3050,  3078,  3147,  3158,  3165,  3172,  3182,\n",
       "         3387,  3435,  3450,  3490,  3627,  3683,  3688,  3730,  3810,\n",
       "         3843,  3848,  3857,  3914,  3952,  4052,  4059,  4130,  4157,\n",
       "         4250,  4326,  4424,  4437,  4613,  4668,  4691,  4769,  4787,\n",
       "         4836,  4870,  4893,  4931,  5035,  5107,  5205,  5286,  5302,\n",
       "         5339,  5428,  5460,  5503,  5543,  5575,  5584,  5603,  5715,\n",
       "         5879,  5891,  6133,  6200,  6223,  6432,  6445,  6490,  6515,\n",
       "         6522,  6525,  6559,  6641,  6714,  6802,  7008,  7040,  7121,\n",
       "         7208,  7337,  7367,  7410,  7419,  7481,  7655,  7660,  7841,\n",
       "         7860,  7945,  7949,  7955,  8007,  8110,  8205,  8226,  8263,\n",
       "         8275,  8319,  8388,  8395,  8416,  8436,  8466,  8510,  8632,\n",
       "         8647,  8808,  8844,  8926,  8934,  9028,  9049,  9148,  9231,\n",
       "         9435,  9463,  9681,  9723,  9805,  9848,  9866,  9918,  9979,\n",
       "        10007, 10047, 10065, 10106, 10108, 10245, 10394, 10472, 10508,\n",
       "        10587, 10601, 10611, 10908, 11160, 11334, 11488, 11522, 11567,\n",
       "        11618, 11636, 11690, 11726, 11797, 11828, 11838, 11845, 11933,\n",
       "        11943, 11953, 12066, 12076, 12215, 12309, 12488, 12496, 12635,\n",
       "        12686, 12797, 12799, 12923, 12938, 13139, 13231, 13313, 13315,\n",
       "        13614, 13654, 13663, 13721, 13847, 14111, 14122, 14166, 14180,\n",
       "        14444, 14529, 14568, 14595, 14611, 14645, 14794, 14809, 14821,\n",
       "        14899, 14932, 15004, 15026, 15129, 15365, 15493, 15507, 15794,\n",
       "        15820, 15940, 16064, 16253, 16345, 16422, 16467, 16617, 16636,\n",
       "        16656, 16703, 16724, 16824, 16842, 16866, 16869, 16918, 17049,\n",
       "        17231, 17239, 17370, 17375, 17451, 17464, 17486, 17550, 17596,\n",
       "        17777, 17795, 17935, 17943, 18269, 18435, 18484, 18506, 18536,\n",
       "        18544, 18691, 18797, 19075, 19182, 19200, 19201, 19293, 19437,\n",
       "        19574, 19643, 19673, 19691, 19934, 20055, 20065, 20081, 20207,\n",
       "        20224, 20266, 20308, 20443, 20634, 20651, 20703, 20739, 20778,\n",
       "        20821, 21109, 21193, 21195, 21198, 21219, 21254, 21280, 21319,\n",
       "        21509, 21527, 21554, 21602, 21704, 21951, 21990, 22139, 22144,\n",
       "        22156, 22264, 22388, 22496, 22506, 22516, 22524, 22531, 22586,\n",
       "        22588, 22773, 22783, 22795, 22810, 22846, 22874, 22880, 22887,\n",
       "        22958, 22968, 22994, 23037, 23242, 23493, 23716, 23730, 23872,\n",
       "        23877, 23884, 23994, 24203, 24215, 24265, 24318, 24481, 24518,\n",
       "        24531, 24561, 24639, 24641, 24650, 24740, 24741, 24788, 24847,\n",
       "        25060, 25092, 25112, 25206, 25251, 25269, 25285, 25288, 25319,\n",
       "        25320, 25372, 25455, 25571, 25579, 25582, 25589, 25603, 25694,\n",
       "        25703, 25712, 25742, 25747, 25882, 25907, 25960, 26120, 26128,\n",
       "        26164, 26244, 26293, 26308, 26352, 26373, 26436, 26459, 26599,\n",
       "        26721, 26781, 26896, 26910, 26972, 26991, 27243, 27276, 27288,\n",
       "        27327, 27436, 27569, 27581, 27831, 27879, 27973, 28025, 28069,\n",
       "        28097, 28160, 28190, 28287, 28304, 28320, 28326, 28342, 28350,\n",
       "        28419, 28488, 28525, 28596, 28690, 28746, 28752, 28756, 28789,\n",
       "        28899, 28964, 29011, 29022, 29382, 29569, 29661, 29664, 29681,\n",
       "        29712, 29838, 29887, 29920, 29981, 30082, 30374, 30396, 30408,\n",
       "        30431, 30564, 30602, 30633, 30673, 30675, 30848, 30857, 30888,\n",
       "        30923, 30936, 30955, 31029, 31050, 31065, 31124, 31234, 31350,\n",
       "        31447, 31660, 31748, 31783, 31802, 31838, 31937, 31965, 32054,\n",
       "        32233, 32304, 32308, 32626, 32644, 32717, 32772, 32773, 32858,\n",
       "        32892, 32911, 33132, 33172, 33190, 33246, 33290, 33522, 33544,\n",
       "        33753, 33814, 33907, 34069, 34081, 34090, 34127, 34187, 34239,\n",
       "        34419, 34604, 34752, 34830, 34846, 34851, 34875, 34976, 35095,\n",
       "        35130, 35744, 35753, 35791, 35799, 35987, 36068, 36259, 36477,\n",
       "        36570, 36577, 36624, 36630, 36683, 36732, 36760, 36911, 36946,\n",
       "        36975, 36979, 37060, 37222, 37225, 37437, 37495, 37609, 37737,\n",
       "        37802, 37928, 38023, 38026, 38061, 38247, 38279, 38298, 38311,\n",
       "        38359, 38484, 38487, 38495, 38642, 38795, 38818, 38830, 38854,\n",
       "        38979, 39048, 39059, 39096, 39290, 39299, 39370, 39403, 39510,\n",
       "        39548, 39554, 39842, 39998, 40154, 40236, 40424, 40549, 40576,\n",
       "        40722, 40771, 40841, 40847, 40910, 40921, 40925, 40936, 40980,\n",
       "        41057, 41098, 41117, 41138, 41163, 41483, 41567, 41575, 41624,\n",
       "        41654, 41711, 41779, 41815, 41928, 41935, 42008, 42127, 42167,\n",
       "        42172, 42215, 42225, 42239, 42300, 42474, 42564, 42643, 42649,\n",
       "        42698, 42782, 42852, 42913, 42947, 42981, 43108, 43157, 43158,\n",
       "        43160, 43165, 43225, 43251, 43289, 43429, 43430, 43446, 43466,\n",
       "        43501, 43620, 43676, 43690, 43747, 43819, 43821, 43853, 43951,\n",
       "        43985, 44053, 44098, 44108, 44139, 44145, 44185, 44325, 44340,\n",
       "        44370, 44413, 44706, 44721, 44730, 44737, 44845, 44972, 44981,\n",
       "        45130, 45214, 45357, 45596, 45633, 45747, 45751, 45838, 45844,\n",
       "        45946, 45982, 45983, 46000, 46003, 46037, 46102, 46137, 46151,\n",
       "        46161, 46275, 46322, 46373, 46381, 46461, 46463, 46481, 46524,\n",
       "        46643, 46662, 46807, 46836, 46862, 47034, 47071, 47100, 47201,\n",
       "        47291, 47294, 47322, 47409, 47599, 47601, 47615, 47657, 47688,\n",
       "        48158, 48261, 48375, 48380, 48394, 48423, 48509, 48531, 48618,\n",
       "        48619, 48623, 48680, 48682, 48803, 48833, 48840, 49077, 49153,\n",
       "        49157, 49235, 49467, 49481, 49623, 49732, 49924, 49945, 49956,\n",
       "        49958, 49970, 49980, 50027, 50115, 50171, 50501, 50654, 50735,\n",
       "        50756, 50812, 50883, 51052, 51053, 51063, 51078, 51149, 51172,\n",
       "        51186, 51522, 51746, 51789, 51844, 51908, 51929, 51966, 51984,\n",
       "        51989, 52071, 52217, 52303, 52329, 52376, 52395, 52528, 52555,\n",
       "        52597, 52599, 52653, 52893, 52932, 52991, 53053, 53071, 53153,\n",
       "        53156, 53158, 53271, 53371, 53490, 53512, 53571, 53728, 53729,\n",
       "        53774, 53859, 53882, 53892, 54079, 54148, 54258, 54362, 54390,\n",
       "        54644, 54684, 54699, 54744, 54813, 54825, 54914, 54960, 55006,\n",
       "        55108, 55164, 55406, 55409, 55527, 55650, 55670, 55679, 55681,\n",
       "        55858, 55934, 55969, 55970, 56043, 56067, 56127, 56155, 56270,\n",
       "        56291, 56315, 56419, 56455, 56472, 56671, 56682, 56696, 56724,\n",
       "        56752, 56783, 56785, 56851, 56868, 56912, 56949, 57047, 57061,\n",
       "        57436, 57444, 57479, 57543, 57554, 57649, 57651, 57682, 57823,\n",
       "        57839, 57907, 58097, 58145, 58165, 58188, 58405, 58669, 58682,\n",
       "        59024, 59040, 59047, 59080, 59092, 59195, 59285, 59420, 59421,\n",
       "        59586, 59596, 59632, 59700, 59818, 59995, 60095, 60120, 60187,\n",
       "        60252, 60306, 60457, 60506, 60520, 60554, 60726, 60835, 60846,\n",
       "        60868, 60979, 61123, 61134, 61178, 61217, 61511, 61514, 61678,\n",
       "        61739, 61749, 61753, 61798, 61802, 61810, 61874, 61973, 62007,\n",
       "        62016, 62210, 62229, 62276, 62303, 62366, 62480, 62545, 62562,\n",
       "        62684, 62749, 62961, 63082, 63183, 63346, 63450, 63747, 63782,\n",
       "        63826, 63923, 63932, 63945, 64005, 64097, 64296, 64484, 64499,\n",
       "        64616, 64622, 64688, 64772, 64868, 64881, 64894, 64980, 65011,\n",
       "        65070, 65149, 65188, 65233, 65254, 65268, 65377, 65415, 65446,\n",
       "        65640, 65655, 65687, 65770, 65800, 65822, 65873, 65921, 65944,\n",
       "        66005, 66215, 66217, 66279, 66331, 66343, 66422, 66465, 66480,\n",
       "        66490, 66581, 66644, 66774, 66837, 66947, 66977, 67046, 67114,\n",
       "        67169, 67205, 67238, 67253, 67296, 67364, 67743, 67803, 67830,\n",
       "        68024, 68169, 68279, 68312, 68340, 68419, 68472, 68742, 68815,\n",
       "        68843, 68859, 68898, 69008, 69026, 69031, 69131, 69215, 69241,\n",
       "        69443, 69503, 69615, 69617, 69786, 69814, 69832, 69840, 69855,\n",
       "        69919, 70039, 70076, 70096, 70194, 70233, 70336, 70361, 70369,\n",
       "        70428, 70506, 70586, 70590, 70672, 70759, 70765, 70948, 71026,\n",
       "        71187, 71230, 71233, 71242, 71411, 71451, 71754, 71895, 71929,\n",
       "        72016, 72081, 72171, 72210, 72258, 72313, 72511, 72606, 72720,\n",
       "        72949]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['sharedix']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
